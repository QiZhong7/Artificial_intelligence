{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>CS4618: Artificial Intelligence I</h1>\n",
    "<h1>Datasets</h1>\n",
    "<h2>\n",
    "    Derek Bridge<br>\n",
    "    School of Computer Science and Information Technology<br>\n",
    "    University College Cork\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<h1>Initialization</h1>\n",
    "$\\newcommand{\\Set}[1]{\\{#1\\}}$ \n",
    "$\\newcommand{\\Tuple}[1]{\\langle#1\\rangle}$ \n",
    "$\\newcommand{\\v}[1]{\\pmb{#1}}$ \n",
    "$\\newcommand{\\cv}[1]{\\begin{bmatrix}#1\\end{bmatrix}}$ \n",
    "$\\newcommand{\\rv}[1]{[#1]}$ \n",
    "$\\DeclareMathOperator{\\argmax}{arg\\,max}$ \n",
    "$\\DeclareMathOperator{\\argmin}{arg\\,min}$ \n",
    "$\\DeclareMathOperator{\\dist}{dist}$\n",
    "$\\DeclareMathOperator{\\abs}{abs}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from numpy.random import rand\n",
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Features</h1>\n",
    "<ul>\n",
    "    <li>Suppose we want to store data about objects, such as houses.</li>\n",
    "    <li><b>Features</b> describe the houses, e.g.\n",
    "        <ul>\n",
    "            <li>$\\mathit{flarea}$: the total floor area (in square metres);</li>\n",
    "            <li>$\\mathit{bdrms}$: the number of bedrooms;</li>\n",
    "            <li> $\\mathit{bthrms}$: the number of bathrooms.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>A particular house has <b>values</b> for the features:\n",
    "        <ul>\n",
    "            <li>e.g. your house: $\\mathit{flarea} = 126, \\mathit{bdrms} = 3, \\mathit{bthrms} = 1$.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Then we can represent a house using a vector:\n",
    "        <ul>\n",
    "            <li>e.g. your house: $\\cv{126\\\\3\\\\1}$\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>We will always use $n$ to refer to the number of features, e.g. above $n = 3$.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Examples</h1> \n",
    "<ul>\n",
    "    <li>Suppose we collect a <b>dataset</b> containing data about lots of houses, e.g.:\n",
    "        $$\\cv{126\\\\3\\\\1} \\,\\, \\cv{92.9\\\\3\\\\2} \\,\\,\\cv{171.9\\\\4\\\\3} \\,\\, \\cv{79\\\\3\\\\1}$$\n",
    "    </li>\n",
    "    <li>Each member of this dataset is called an <b>example</b>, and we will use $m$ to refer to the number of examples, e.g.\n",
    "        above $m = 4$.\n",
    "    </li>\n",
    "<ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Dataset notation</h1>\n",
    "<ul>\n",
    "    <li>We will use a <em>superscript</em> to index the examples.\n",
    "        <ul>\n",
    "            <li>\n",
    "                $\\v{x}^{(i)}$ will be the $i$th example.\n",
    "            </li>\n",
    "            <li>\n",
    "                The first example in the dataset is $\\v{x}^{(1)}$, the second is $\\v{x}^{(2)}$, $\\ldots$, \n",
    "                the last is $\\v{x}^{(m)}$ (Note, we index from 1.)\n",
    "            </li>\n",
    "            <li>\n",
    "                We're writing the superscript in parentheses to make it clear that we are using it for indexing.\n",
    "                It is not 'raising to a power'. If we want to raise to a power, we will drop the parentheses.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>We will use a <em>subscript</em> to index the features (again starting from 1).</li>\n",
    "    <li>Class exercise. Using the dataset from above:\n",
    "        <ul>\n",
    "            <li>what is $\\v{x}_2^{(1)}$?</li>\n",
    "            <li>what is $\\v{x}_1^{(2)}$?</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Dataset as a matrix</h1>\n",
    "<ul>\n",
    "    <li>We can represent a dataset $\\Set{\\v{x}^{(1)}, \\v{x}^{(2)}, \\ldots, \\v{x}^{(m)}}$ as a $m \\times n$\n",
    "        matrix $\\v{X}$ as follows:\n",
    "        $$\\v{X} = \\begin{bmatrix}\n",
    "              \\v{x}_1^{(1)} & \\v{x}_2^{(1)} & \\ldots & \\v{x}_n^{(1)} \\\\\n",
    "              \\v{x}_1^{(2)} & \\v{x}_2^{(2)} & \\ldots & \\v{x}_n^{(2)} \\\\\n",
    "              \\vdots        & \\vdots        & \\vdots & \\vdots \\\\\n",
    "              \\v{x}_1^{(m)} & \\v{x}_2^{(m)} & \\ldots & \\v{x}_n^{(m)} \\\\\n",
    "              \\end{bmatrix}\n",
    "        $$\n",
    "    </li>\n",
    "    <li>Note how each example becomes a <em>row</em> in $\\v{X}$.</li>\n",
    "    <li>You can think of row $i$ as the transpose of $\\v{x}^{(i)}$.</li>\n",
    "    <li>For the example dataset, we get:\n",
    "        $$\\v{X} = \n",
    "            \\begin{bmatrix}\n",
    "                126 & 3 & 1 \\\\\n",
    "                92.9 & 3 & 2 \\\\\n",
    "                171.9 & 4 & 3 \\\\\n",
    "                79 & 3 & 1\n",
    "            \\end{bmatrix}\n",
    "        $$\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Cork Property Prices Dataset</h1>\n",
    "<ul>\n",
    "    <li>In August 2019, I scraped a dataset of property prices for Cork city from www.daft.ie.</li>\n",
    "    <li>They are in a CSV file. Each line in the file is an example, representing one house.</li>\n",
    "    <li>Hence, each line of the file contains the feature-values for the floor area, number of bedrooms, number of\n",
    "        bathrooms, and several other features that we will ignore for now.\n",
    "    </li>\n",
    "    <li>We will use the pandas library:\n",
    "        <ul>\n",
    "            <li>to read the dataset from the csv file into what pandas calls a DataFrame;</li>\n",
    "            <li>to explore the dataset: looking at values and computing summary statistics.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Then we will extract some of the features (columns) and convert to a numpy 2D array, before using the data\n",
    "        to find houses similar to yours.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Using pandas to Read and Explore the Data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pandas to read the CSV file into a DataFrame\n",
    "df = pd.read_csv(\"../datasets/dataset_corkA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(464, 4)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The dimensions\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['flarea', 'bdrms', 'bthrms', 'price'], dtype='object')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The features\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flarea    float64\n",
       "bdrms       int64\n",
       "bthrms      int64\n",
       "price       int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The datatypes\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flarea</th>\n",
       "      <th>bdrms</th>\n",
       "      <th>bthrms</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>464.000000</td>\n",
       "      <td>464.000000</td>\n",
       "      <td>464.000000</td>\n",
       "      <td>464.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>125.460151</td>\n",
       "      <td>3.329741</td>\n",
       "      <td>2.120690</td>\n",
       "      <td>352.297414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>70.692202</td>\n",
       "      <td>1.068445</td>\n",
       "      <td>1.061033</td>\n",
       "      <td>197.464495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>40.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>95.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>82.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>235.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>110.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>295.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>140.600000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>395.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>575.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1495.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           flarea       bdrms      bthrms        price\n",
       "count  464.000000  464.000000  464.000000   464.000000\n",
       "mean   125.460151    3.329741    2.120690   352.297414\n",
       "std     70.692202    1.068445    1.061033   197.464495\n",
       "min     40.000000    1.000000    1.000000    95.000000\n",
       "25%     82.000000    3.000000    1.000000   235.000000\n",
       "50%    110.000000    3.000000    2.000000   295.000000\n",
       "75%    140.600000    4.000000    3.000000   395.000000\n",
       "max    575.000000    9.000000    6.000000  1495.000000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary statistics\n",
    "df.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flarea</th>\n",
       "      <th>bdrms</th>\n",
       "      <th>bthrms</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>111.9</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>95.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>120.8</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>275</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   flarea  bdrms  bthrms  price\n",
       "0   111.9      3       3    305\n",
       "1    95.0      3       3    255\n",
       "2   120.8      3       3    275"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A few of the examples\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Convert to a numpy 2D array</h2>\n",
    "<ul>\n",
    "    <li>We will select certain features (columns) from the pandas DataFrame\n",
    "        and convert to a 2D numpy array\n",
    "    </li>\n",
    "    <li>(Later in the module, we will use a <code>ColumnTransformer</code> to do this.)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The features we want to select\n",
    "features = [\"flarea\", \"bdrms\", \"bthrms\"]\n",
    "\n",
    "# Extract these features and convert to numy 2D array\n",
    "X = df[features].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[111.9,   3. ,   3. ],\n",
       "       [ 95. ,   3. ,   3. ],\n",
       "       [120.8,   3. ,   3. ]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at a few rows in X - to show you that we now have a 2D numpy array\n",
    "X[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Similarity &amp; Distance</h1>\n",
    "<ul>\n",
    "    <li>In AI, we often want to know how <em>similar</em> one object is to another.\n",
    "        <ul>\n",
    "            <li>E.g. how similar is my house to yours?</li>\n",
    "            <li>E.g. which house in our dataset is most similar to yours?</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>In fact, here we are instead going to measure how <em>different</em> they are using a <b>distance function</b>.\n",
    "        <ul>\n",
    "            <li>(N.B. This is not about geographical distance.)</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Let $\\v{x}$ be one vector of feature values and $\\v{x}'$ be another.</li>\n",
    "    <li>Simplest is to measure their <b>Euclidean distance</b>:\n",
    "        $$d(\\v{x}, \\v{x}') = \\sqrt{(\\v{x}_1 - \\v{x}_1')^2 + (\\v{x}_2 - \\v{x}_2')^2 + \\ldots + (\\v{x}_n - \\v{x}_n')^2}$$\n",
    "        or, more concisely:\n",
    "        $$d(\\v{x}, \\v{x}') = \\sqrt{\\sum_{j=1}^n(\\v{x}_j - \\v{x}_j')^2}$$\n",
    "    </li>\n",
    "    <li>Euclidean distance has a minimum value of 0 (meaning identical) but no maximum value (depends on your data).</li>\n",
    "    <li>Class exercise. What is the Euclidean distance between $\\v{x} = \\cv{100\\\\1\\\\4}$ and $\\v{x}' = \\cv{100\\\\5\\\\1}$?</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Euclidean Distance in numpy</h2>\n",
    "<ul>\n",
    "    <li>It has a nice vectorized implementation (no loop!) using numpy:</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euc(x, xprime):\n",
    "    return np.sqrt(np.sum((x - xprime)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.026297590440446"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example\n",
    "your_house = np.array([126.0, 3, 1])\n",
    "my_house = np.array([107.0, 2, 1])\n",
    "\n",
    "euc(your_house, my_house)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>We can compute the distance between your house and all the houses in X.</li>\n",
    "    <li>(We have to write a loop here, because our <code>euc</code> function is not vectorized.)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists = [euc(your_house, x) for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14.241137595009741, 31.064449134018133, 5.571355310873651]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just to show you, here are the first 3 distances\n",
    "dists[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0332473082471605"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Even better, we can, with one line of code, find the most similar house\n",
    "np.min([euc(your_house, x) for x in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Even better again, we can find which house is the most similar\n",
    "np.argmin([euc(your_house, x) for x in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flarea    125.74\n",
       "bdrms       3.00\n",
       "bthrms      2.00\n",
       "price     398.00\n",
       "Name: 196, dtype: float64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best of all, we can display the most similar house\n",
    "df.iloc[np.argmin([euc(your_house, x) for x in X])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Problems with Euclidean distance</h1>\n",
    "<ul>\n",
    "    <li>There are at least two problems with Euclidean distance (and many other distance measures too):\n",
    "        <ul>\n",
    "            <li>Features with different scales;</li>\n",
    "            <li>Features that are correlated with each other;</li>\n",
    "            <li>The curse of dimensionality.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Scaling Numeric Values</h1>\n",
    "<ul>\n",
    "    <li>Different numeric-valued features often have very different ranges.\n",
    "        <ul>\n",
    "            <li>E.g. the values for floor area are going to range from a few tens to a few hundreds of square metres.</li>\n",
    "            <li>But the number of bedrooms and bathrooms is going to range from 0 to a dozen or so at most.\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>\n",
    "        When computing the Euclidean distance, features with large ranges will dominate the distance calculations, \n",
    "        thus giving features with small ranges negligible influence.\n",
    "    </li>\n",
    "    <li>\n",
    "        E.g., consider your house $\\v{x} = \\cv{126\\\\3\\\\1}$ and two others, $\\v{y} = \\cv{131\\\\3\\\\1}$ and\n",
    "        $\\v{z} = \\cv{126\\\\7\\\\1}$. \n",
    "        <ul>\n",
    "            <li><em>Intuitively</em>, which house is more similar to yours, $\\v{y}$ or $\\v{z}$?</li>\n",
    "            <li>Now compute the Euclidean distances.</li>\n",
    "            <li>According to these distances, which house is more similar to yours?</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>\n",
    "        The solution is to <b>scale</b> (or 'normalize') the values so that they have similar ranges.\n",
    "    </li>\n",
    "    <li>There are several ways to do this. One is <b>min-max scaling</b>, but the one we'll discuss is <b>standardization</b>.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Standardization</h2>\n",
    "<ul>\n",
    "    <!--\n",
    "    <li>In some cases, you don't want feature values to have the same range but to have the same mean\n",
    "        and even the same variance\n",
    "    </li>\n",
    "    -->\n",
    "    <li>\n",
    "        One idea is <b>mean centering</b>, where you subtract the mean value of the feature.\n",
    "        <ul>\n",
    "            <li>If you do this to all values, some of the new values will be positive and some will be negative and \n",
    "                their mean will be approximately zero.\n",
    "                </li>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>But better still is <b>standardization</b>, in which you subtract the mean and divide by the standard\n",
    "        deviation:\n",
    "        $$\\v{x}_j \\gets \\frac{\\v{x}_j - \\mu_j}{\\sigma_j}$$\n",
    "        where $\\mu_j$ is the mean of the values for feature $j$ and $\\sigma_j$ is their standard deviation\n",
    "    </li>\n",
    "    <li>\n",
    "        If you use this, then the mean will be approximately zero, the standard deviation will be 1.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Standardization in scikit-learn</h2>\n",
    "<ul>\n",
    "    <li>scikit-learn provides a class called <code>StandardScaler</code>.\n",
    "    </li>\n",
    "    <li>It uses means and standard deviations that it calculates from your dataset. (Statisticians would say that it should\n",
    "        use the population mean and standard deviation, but these are generally not known.)\n",
    "    </li>\n",
    "    <li>We create the scaler and then run its <code>fit</code> and <code>transform</code> methods.</li>\n",
    "    <li>(Later in the module, when we are using a <code>ColumnTransformer</code>, running these methods\n",
    "        will be done for us.)\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(X)\n",
    "X_scaled = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.19202665, -0.30895098,  0.82962481],\n",
       "       [-0.43134924, -0.30895098,  0.82962481],\n",
       "       [-0.06599286, -0.30895098,  0.82962481]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at a few rows in X\n",
    "X_scaled[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00764486, -0.30895098, -1.05736495])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's scale your house too\n",
    "# Don't try to understand or copy this code - it's a hack that you won't need\n",
    "your_house = np.array([[126.0, 3, 1]])\n",
    "your_house_scaled = scaler.transform(your_house)[0]\n",
    "your_house_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "328"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To see what effect this has had, let's see which house is most similar to yours\n",
    "np.argmin([euc(your_house_scaled, x) for x in X_scaled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flarea    122.4\n",
       "bdrms       3.0\n",
       "bthrms      1.0\n",
       "price     295.0\n",
       "Name: 328, dtype: float64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[np.argmin([euc(your_house_scaled, x) for x in X_scaled])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Features that are Correlated</h1>\n",
    "<ul>\n",
    "    <li>Let's start with an extreme example. \n",
    "        <ul>\n",
    "            <li>Suppose one feature is the floor area in square metres and\n",
    "                another is the floor area in square feet.\n",
    "                Then it's clear that, even after scaling, when calculating distances, floor area will have greater\n",
    "                influence than other features, such as the number of bedrooms, because it is in the dataset twice.\n",
    "            </li>\n",
    "            <li>Examples are often less stark. For example, floor area and the number of bedrooms are correlated,\n",
    "                and so their contributions to the distance calculations are not independent of each other.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Ideally the features should be independent (at least, linearly independent).</li>\n",
    "    <li>Yet, few people who use distances do anything about this problem!</li>\n",
    "    <li>Solutions (which we're not covering in detail) include feature weighting and projections to\n",
    "        a new feature space whose features are (linearly) independent (e.g. using Principal Component\n",
    "        Analysis).\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>The Curse of Dimensionality</h1>\n",
    "<ul>\n",
    "    <li>In some datasets, examples have thousands or even millions of features.\n",
    "        <ul>\n",
    "            <li>E.g. datasets from astronomy;</li>\n",
    "            <li>E.g. datasets of images and videos;</li>\n",
    "            <li>E.g. datasets of documents where each unique word is a feature.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Is it better or worse to have more features?\n",
    "        <ul>\n",
    "            <li>Storage and processing costs increase.</li>\n",
    "            <li>Apart from efficiency, intuitively, more features is better:\n",
    "                <ul>\n",
    "                    <li>e.g. describing houses more completely.</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>But, counter-intuitively, that isn't true in general.\n",
    "                <ul>\n",
    "                    <li>As the number of features grows, algorithms that use distance and density, will find it harder \n",
    "                        to find good solutions.\n",
    "                    </li>\n",
    "                    <li>The problems that arise as the number of features grows have been called <b>the curse of dimensionality</b>.\n",
    "    </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Example of the Curse of Dimensionality</h2>\n",
    "<ul>\n",
    "    <li>The code that follows (which you don't need to study):\n",
    "        <ul>\n",
    "            <li>generates a random dataset where $m = 400$ and $n = 2$ and both features have values in $[0, 1)$;\n",
    "            </li>\n",
    "            <li>computes the Euclidean distance between all pairs of examples;</li>\n",
    "            <li>finds $d_{\\mathit{min}}$, the smallest of these distances;</li>\n",
    "            <li>finds $d_{\\mathit{max}}$, the largest of the distances;</li>\n",
    "            <li>computes the ratio $\\frac{d_{\\mathit{max}}}{d_{\\mathit{min}}}$.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>It then does this all again but with $n = 3, 4, 5,\\ldots,500$.</li>\n",
    "    <li>Then it plots the ratios that it has computed ($y$-axis, but note its scale) against $n$ ($x$-axis).</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "m = 400\n",
    "n_range = range(1, 500)\n",
    "\n",
    "ratios = []\n",
    "for n in n_range:\n",
    "    X = rand(m, n)\n",
    "    dists = euclidean_distances(X)\n",
    "    non_zero_dists = dists[dists > 0]\n",
    "    ratios += [np.max(non_zero_dists) / (np.min(non_zero_dists))]\n",
    "    \n",
    "fig = plt.figure(figsize=(12,6))\n",
    "plt.title(\"Ratio of max distance to min distance in datasets with ever more features\")\n",
    "plt.scatter(n_range, ratios)\n",
    "plt.yscale('log')\n",
    "plt.xlabel(\"n\")\n",
    "plt.xlim(0, 500)\n",
    "plt.ylabel(\"ratio\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>As $n \\rightarrow \\infty$, $d_{\\mathit{max}} \\rightarrow d_{\\mathit{min}}$, so their ratio tends to 1.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.262856707214241,\n",
       " 1.2571366467295468,\n",
       " 1.2587866883712442,\n",
       " 1.2553439511765352,\n",
       " 1.236453301246161]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since it may not be clear from the graph, we'll show the last 5 of the ratios that it calculated\n",
    "ratios[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>We conclude (counter-intutively) that examples become equi-distant!</li>\n",
    "    <li>This obviously undermines methods that depend on finding objects that are similar to each other, as we were\n",
    "        doing earlier &mdash; with more features, the most similar object becomes more arbitrary!\n",
    "    </li>\n",
    "    <li>The problem extends to other distance/similarity measures, e.g. cosine similarity.</li>\n",
    "    <li>Fortunately, there are lots of methods available for reducing dimensionality.\n",
    "        One solution is to retain the principle components found by Principal Component Analysis. This is\n",
    "        interesting because PCA was suggested above as a solution to the problem of correlated features. \n",
    "        It can actually help us solve both problems.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
