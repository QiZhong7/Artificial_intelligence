{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>CS4618: Artificial Intelligence I</h1>\n",
    "<h1>Neural Network Architectures</h1>\n",
    "<h2>\n",
    "    Derek Bridge<br>\n",
    "    School of Computer Science and Information Technology<br>\n",
    "    University College Cork\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Initialization</h1>\n",
    "$\\newcommand{\\Set}[1]{\\{#1\\}}$ \n",
    "$\\newcommand{\\Tuple}[1]{\\langle#1\\rangle}$ \n",
    "$\\newcommand{\\v}[1]{\\pmb{#1}}$ \n",
    "$\\newcommand{\\cv}[1]{\\begin{bmatrix}#1\\end{bmatrix}}$ \n",
    "$\\newcommand{\\rv}[1]{[#1]}$ \n",
    "$\\DeclareMathOperator{\\argmax}{arg\\,max}$ \n",
    "$\\DeclareMathOperator{\\argmin}{arg\\,min}$ \n",
    "$\\DeclareMathOperator{\\dist}{dist}$\n",
    "$\\DeclareMathOperator{\\abs}{abs}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Acknowledgments</h1>\n",
    "<ul>\n",
    "    <li>The diagrams and code are based on diagrams and code in: A. G&eacute;ron: \n",
    "        <i>Hands-On Machine Learning with Scikit-Learn, Keras and TensorFlow (2nd edn)</i>, O'Reilly, 2019\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Warning</h1>\n",
    "<ul>\n",
    "    <li>The code does not run. I'm providing snippets only.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Introduction</h1>\n",
    "<ul>\n",
    "    <li>Mostly, we've been looking at quite simple neural network architectures: stacks of layers.\n",
    "        <ul>\n",
    "            <li>In Keras, <code>Sequential</code> models are ideal for this.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>But, the architecture could be more complicated\n",
    "        <ul>\n",
    "            <li>E.g. you might want multiple inputs:\n",
    "                <img src=\"images/mult_inputs_nn.png\" /> \n",
    "                This might be useful if your dataset comprises images and text, which might\n",
    "                best be handled by different subnetworks.\n",
    "            </li>\n",
    "            <li>E.g. you might want multiple outputs:\n",
    "                <img src=\"images/mult_outputs_nn.png\" />\n",
    "                This might be useful if, from pictures of faces, you want to classify the expression\n",
    "                (similing, surprised) but you also want to classify by eyeware (wearing glasses or not).\n",
    "            </li>\n",
    "            <li>In Keras, the <code>Model</code> class makes this possible.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>We'll look at some examples. We won't do all the code (just snippets), so don't try to execute\n",
    "        them. And you don't need to \n",
    "        learn the code.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Classification &amp; Localization, and Object Detection</h1>\n",
    "<ul>\n",
    "    <li>You may want to <b>locate</b> and <b>classify</b> the main object in a picture.\n",
    "        <ul>\n",
    "            <li>This is a classification task: what kind of object is in the image (cat, dog, &hellip;)</li>\n",
    "            <li>It is also a regression task. In fact, you will want to predict four numbers that describe\n",
    "                a bounding box around the object:\n",
    "                <ul>\n",
    "                    <li>$x$-coordinate of the centre of the bounding box;</li>\n",
    "                    <li>$y$-coordinate of the centre of the bounding box;</li>\n",
    "                    <li>width of the bounding box;</li>\n",
    "                    <li>height of the bounding box.</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Hence, this requires a network with multiple outputs.Â How many outputs?</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50_base = ResNet50(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "inputs = Input(shape=(224, 224, 3))\n",
    "x = preprocess_input(inputs)\n",
    "x = resnet50_base(x)\n",
    "class_outputs = Dense(n_classes, activation=\"softmax\")(x)\n",
    "location_outputs = Dense(4, activation=\"linear\")(x)\n",
    "model = Model(inputs, outputs=[class_outputs, location_outputs])\n",
    "\n",
    "model.compile(optimizer=RMSprop(lr=0.00003),\n",
    "              loss=[\"sparse_categorical_cross_entropy\", \"mse\"],\n",
    "              loss_weights=[0.7, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>In this snippet you can see the <code>Model</code> class has multiple outputs.</li>\n",
    "    <li>,And you can see that each output requires its own loss function.\n",
    "        <ul>\n",
    "            <li>By default, Keras sums the losses.</li>\n",
    "            <li>If you care more about one loss than another, you supply weights.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Getting a labelled dataset is now even harder: every image must come with the class and bounding box\n",
    "        of the main object in the image. \n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>What we've been discussing is classification and localization of the <em>main object</em> in an image.\n",
    "        But we can use convolutional neural networks for <b>object detection</b>, which refers to classifying \n",
    "        and localizing <em>multiple objects</em> in an image, e.g. to\n",
    "        say that an image contains a car and a pedestrian and to give bounding boxes for each.\n",
    "    </li>\n",
    "    <li>One way to do object detection:\n",
    "        <ul>\n",
    "            <li>Train a convolutional neural network to classify and locate a single object.</li>\n",
    "            <li>Slide it across the image, i.e. predict and shift, e.g., for each $3\\times 3$ region.</li>\n",
    "            <li>Perhaps repeat this for each $4 \\times 4$ region.</li>\n",
    "            <li>Post-process the results because you will have detected the same object mutliple times.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>There are other clever ways of doing object detection, but we don't have time to look at them\n",
    "        (look up Fully Convolutional Networks and YOLO, if you are interested).\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Autoencoders</h1>\n",
    "<ul>\n",
    "    <li>Autoencoders learn to copy their inputs to their outputs.\n",
    "        <ul>\n",
    "            <li>They have the same number of outputs as inputs.</li>\n",
    "            <li>Reconstruction loss: They are trained with a loss function that penalizes them if the output \n",
    "                for each example is\n",
    "                not the same as the input.\n",
    "            </li>\n",
    "            <li>The network will have constraints that prevent the autoencoder from simply copying inputs to\n",
    "                outputs; the constraints force the autoencoder \n",
    "                to learn an efficient representation, rather than just memorizing; the autoencoder must\n",
    "                find features that capture the inputs and allow it to reconstruct them as outputs.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Stacked autoencoders, for example, have\n",
    "        <ul>\n",
    "            <li>an encoder: layers that convert the input to a more compact internal representation; and</li>\n",
    "            <li>a decoder: layers that convert the internal representation to the outputs.</li>\n",
    "        </ul>\n",
    "        <img src=\"images/stacked_autoencoder.png\" />\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_encoder = Sequential([\n",
    "    Dense(100, activation=\"relu\", input=(200,)), \n",
    "    Dense(30, activation=\"relu\") \n",
    "])\n",
    "stacked_decoder = Sequential([\n",
    "    Dense(100, activation=\"relu\", input_shape=[30]),\n",
    "    Dense(200, activation=\"linear\")\n",
    "])\n",
    "stacked_autoencoder = Sequential([stacked_encoder, stacked_decoder])\n",
    "\n",
    "stacked_autoencoder.compile(optimizer=RMSprop(lr=0.00003), loss=\"mse\") \n",
    "\n",
    "stacked_autoencoder.fit(X_train, X_train, epochs=10, validation_data=[X_valid, X_valid])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>In the snippet, we create two submodels (encoder and decoder) and combine.\n",
    "    </li>\n",
    "    <li>I have pretended we have 200 numeric-valued features. Hence, I use linear as the activation function\n",
    "        on the output layer. Hence, also mse\n",
    "        is a suitable loss function.\n",
    "    </li>\n",
    "    <li>Note that <code>X_train</code> is used for both the inputs and the targets.</li>\n",
    "    <li>Autoencoders have several uses, and we will look at one.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Unsupervised Pretraining using Stacked Autoencoders</h2>\n",
    "<ul>\n",
    "    <li>Suppose you want to build, e.g., an image classifier.</li>\n",
    "    <li>Suppose you have lots of unlabeled data and a little labeled data.\n",
    "        <ul>\n",
    "            <li>E.g. you've downloaded millions of images from the web, but you've manually labeled only\n",
    "                a small subset.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>First train a stacked autoencoder on all your data: hopefully you learn an autoencoder that \n",
    "        is good at detecting features \n",
    "        in the images.\n",
    "    </li>\n",
    "    <li>Then reuse its lower layers (the encoder) like we reused lower layers of a pretrained network in a previous lecture:\n",
    "        <ul>\n",
    "            <li>Create a network that has the lower layers of the autoencoder and then a few additional\n",
    "                layers that implement a classifier.\n",
    "            </li>\n",
    "            <li>Train this new network on the labeled data, but with all or most of the encoder's weights frozen.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Some Other Autoencoders</h2>\n",
    "<ul>\n",
    "    <li>We force stacked autoencoders to learn useful features by giving the internal layers lower\n",
    "        dimensionality. But there are other ways of constructing autoencoders.\n",
    "    </li>\n",
    "    <li>In a denoising autoencoder:\n",
    "        <ul>\n",
    "            <li>all the layers might have the same number of neurons;</li>\n",
    "            <li>to force it to learn useful features, during training noise is added to the inputs;</li>\n",
    "            <li>but the network is trained to reconstruct the noise-free inputs (a bit like dropout).</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>In a sparse autoencoder:\n",
    "        <ul>\n",
    "            <li>a regularization term is added to the loss function to reduce the number of active\n",
    "                neurons;\n",
    "            </li>\n",
    "            <li>this forces the autoencoder to represent each input as a combination of a small number of\n",
    "                activations.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Variational encoders can generate new instances that look like they were sampled from the training\n",
    "        set. However, a new kind of network has become more popular for this &hellip;\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Generative Adversarial Networks</h1>\n",
    "<ul>\n",
    "    <li>Generative Adversarial Networks (GANs) allow for the <em>generation</em> of fairly realistic\n",
    "        synthetic images.\n",
    "    </li>\n",
    "    <li>They comprise:\n",
    "        <ul>\n",
    "            <li>a generator network: takes a random vector (think of it as if it were samples from the\n",
    "                coded representations in the middle of an autoencoder) and decodes it into a synthetic\n",
    "                image; and\n",
    "            </li>\n",
    "            <li>a discriminator network: takes an image which might be real (from the training set) or \n",
    "                might be synthetic (from the generator) and\n",
    "                predicts whether it is real or synthetic.\n",
    "            </li>\n",
    "            Analogy: a forger and an art expert.\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Training:\n",
    "        <ul>\n",
    "            <li>The generator is trained to fool the discriminator, so it must produce ever more realistic\n",
    "                images.\n",
    "            </li>\n",
    "            <li>The discriminator is trained to tell synthetic from real images with high accuracy.</li>\n",
    "        </ul>\n",
    "        As one network gets better, the other will have to get better.\n",
    "        <ul>\n",
    "            <li>Because it's a dynamic system, there isn't a fixed minimum.</li>\n",
    "            <li>Instead of seeking a minimum, we are seeking an equilibrium between two adversaries.</li>\n",
    "            <li>Hence, very difficult to train successfully.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Training (in more detail)</h2>\n",
    "<ul>\n",
    "    <li>In the first phase, we train the discriminator:\n",
    "        <img src=\"images/gan_first.png\" />\n",
    "        <ul>\n",
    "            <li>Sample real images from the training set, labeled 1.</li>\n",
    "            <li>Generate an equal number of synthetic images, labeled 0.</li>\n",
    "            <li>Use an epoch of backprop on the discriminator only with \n",
    "                binary cross-entropy as the loss function.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>In the second phase, we train the generator:\n",
    "        <img src=\"images/gan_second.png\" />\n",
    "        <ul>\n",
    "            <li>Generate some synthetic images, label them all 1 (yes, 1!).</li>\n",
    "            <li>Use an epoch of backprop on the whole GAN with binary cross-entropy as the\n",
    "                loss function but with the weights of the discriminator frozen within the GAN.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>   \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    This code snippets (adapted from Chollet's book, 2nd edn) assume that the coded representation has 128 features \n",
    "    and that the images are $64 \\times 64$.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 128\n",
    "\n",
    "generator = Sequential([\n",
    "    Input(shape=(latent_dim,)),\n",
    "    Dense(8 * 8 * 128),\n",
    "    Reshape((8, 8, 128)),\n",
    "    Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\"),\n",
    "    LeakyReLU(alpha=0.2),\n",
    "    Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\"),\n",
    "    LeakyReLU(alpha=0.2),\n",
    "    Conv2DTranspose(512, kernel_size=4, strides=2, padding=\"same\"),\n",
    "    LeakyReLU(alpha=0.2),\n",
    "    Conv2D(3, kernel_size=5, padding=\"same\", activation=\"sigmoid\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = Sequential([\n",
    "    Input(shape=(64, 64, 3)),\n",
    "    Conv2D(64, kernel_size=4, strides=2, padding=\"same\"),\n",
    "    LeakyReLU(alpha=0.2),\n",
    "    Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
    "    LeakyReLU(alpha=0.2),\n",
    "    Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
    "    LeakyReLU(alpha=0.2),\n",
    "    Flatten(),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation=\"sigmoid\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The regular fit method cannot be used. We need a fit method that call this...\n",
    "        \n",
    "def train_step(real_images):\n",
    "    batch_size = tf.shape(real_images)[0]\n",
    "    random_latent_vectors = tf.random.normal(shape=(batch_size, latent_dim))\n",
    "    generated_images = generator(random_latent_vectors)\n",
    "    combined_images = tf.concat([generated_images, real_images], axis=0)\n",
    "    labels = tf.concat(\n",
    "        [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
    "    )\n",
    "    labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = discriminator(combined_images)\n",
    "        d_loss = loss_fn(labels, predictions)\n",
    "    grads = tape.gradient(d_loss, discriminator.trainable_weights)\n",
    "    d_optimizer.apply_gradients(\n",
    "        zip(grads, discriminator.trainable_weights)\n",
    "    )\n",
    "\n",
    "    random_latent_vectors = tf.random.normal(\n",
    "        shape=(batch_size, latent_dim))\n",
    "\n",
    "    misleading_labels = tf.zeros((batch_size, 1))\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = discriminator(generator(random_latent_vectors))\n",
    "        g_loss = loss_fn(misleading_labels, predictions)\n",
    "    grads = tape.gradient(g_loss, generator.trainable_weights)\n",
    "    g_optimizer.apply_gradients(zip(grads, generator.trainable_weights))\n",
    "\n",
    "    d_loss_metric.update_state(d_loss)\n",
    "    g_loss_metric.update_state(g_loss)\n",
    "    return {\"d_loss\": d_loss_metric.result(), \"g_loss\": g_loss_metric.result()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    The code snippet is just a rough idea. As already said, successful training is hard, and would require\n",
    "    a lot of tweaks, which you can look up if you're interested.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GANs are now used for these and other purposes:\n",
    "<ul>\n",
    "    <li>increasing the resolution of images;</li>\n",
    "    <li>colorization;</li>\n",
    "    <li>image editing, e.g. replacing photo-bombers with realistic backgrounds;</li>\n",
    "    <li>turning sketches into photo-like images;</li>\n",
    "    <li>augmenting image datasets;</li>\n",
    "    <li>&hellip;</li>\n",
    "</ul>\n",
    "<!--\n",
    "https://www.ft.com/content/56dde36c-aa40-11e9-984c-fac8325aaa04 (paywall now)\n",
    "https://www.forbes.com/sites/korihale/2019/05/28/google-microsoft-banking-on-africas-ai-labeling-workforce/?sh=34298493541c\n",
    "https://scale.com/\n",
    "https://www.sama.com/\n",
    "https://www.amazon.co.uk/Old-Ireland-Colour-John-Breslin/dp/1785373706\n",
    "https://www.insight-centre.org/our-team/dr-john-breslin/\n",
    "http://oldirelandincolour.com/\n",
    "https://www.slideshare.net/Cloud/old-ireland-in-colour\n",
    "https://thispersondoesnotexist.com/\n",
    "https://www.whichfaceisreal.com/index.php\n",
    "https://thisrentaldoesnotexist.com/\n",
    "https://thisxdoesnotexist.com/\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    This ends AI1! We start AI2 with lectures on getting computers to understand human languages.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
