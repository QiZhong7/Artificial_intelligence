{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>CS4618: Artificial Intelligence I</h1>\n",
    "<h1>Gradient Descent</h1>\n",
    "<h2>\n",
    "    Derek Bridge<br>\n",
    "    School of Computer Science and Information Technology<br>\n",
    "    University College Cork\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<h1>Initialization</h1>\n",
    "$\\newcommand{\\Set}[1]{\\{#1\\}}$ \n",
    "$\\newcommand{\\Tuple}[1]{\\langle#1\\rangle}$ \n",
    "$\\newcommand{\\v}[1]{\\pmb{#1}}$ \n",
    "$\\newcommand{\\cv}[1]{\\begin{bmatrix}#1\\end{bmatrix}}$ \n",
    "$\\newcommand{\\rv}[1]{[#1]}$ \n",
    "$\\DeclareMathOperator{\\argmax}{arg\\,max}$ \n",
    "$\\DeclareMathOperator{\\argmin}{arg\\,min}$ \n",
    "$\\DeclareMathOperator{\\dist}{dist}$\n",
    "$\\DeclareMathOperator{\\abs}{abs}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ipywidgets import interactive\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.preprocessing import add_dummy_feature\n",
    "\n",
    "from sklearn.linear_model import SGDRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Acknowledgement</h1>\n",
    "<ul>\n",
    "    <li>I based 5 of the diagrams on ones to be found in A. G&eacute;ron: <i>Hands-On Machine Learning with Scikit-Learn, Keras &amp;\n",
    "        TensorFlow (2nd edn)</i>, O'Reilly, 2019\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Gradient Descent</h1>\n",
    "<ul>\n",
    "    <li><b>Gradient Descent</b> is a generic method for finding optimal solutions to problems that involve\n",
    "        minimizing a loss function.\n",
    "    </li>\n",
    "    <li>It is a <em>search</em> in the model's <b>parameter space</b> for values of the parameters that minimize \n",
    "        the loss function.\n",
    "    </li>\n",
    "    <li>Conceptually:\n",
    "        <ul>\n",
    "            <li>\n",
    "                 It starts with an initial guess for the values of the parameters.\n",
    "            </li>\n",
    "            <li>\n",
    "                Then repeatedly:\n",
    "                <ul>\n",
    "                    <li>It updates the parameter values  &mdash; hopefully to reduce the loss.\n",
    "                    </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "        <img src=\"images/fog.jpg\" alt=\"\" />\n",
    "    </li>\n",
    "    <li>\n",
    "        Ideally, it keeps doing this until <b>convergence</b> &mdash; changes to the parameter values do not result\n",
    "        in lower loss.\n",
    "    </li>\n",
    "    <li>The key to this algorithm is how to update the parameter values.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>The update rule</h2>\n",
    "<ul>\n",
    "     <li>To update the parameter values to reduce the loss:\n",
    "         <ul>\n",
    "             <li>Compute the gradient vector.\n",
    "                 <ul>\n",
    "                     <li>But this points 'uphill' and we want to go 'downhill'.</li>\n",
    "                     <li>And we want to make 'baby steps' (see later), so we use a <b>learning rate</b>, \n",
    "                         $\\alpha$, which is between 0 and 1.\n",
    "                     </li>\n",
    "                 </ul>\n",
    "             </li>\n",
    "             <li>So subtract $\\alpha$ times the gradient vector from $\\v{\\beta}$.</li>\n",
    "         </ul>\n",
    "         $$\\v{\\beta} \\gets \\v{\\beta} - \\alpha\\nabla_{\\v{\\beta}}J(\\v{X}, \\v{y}, \\v{\\beta})$$\n",
    "         Or\n",
    "         $$\\v{\\beta} \\gets \\v{\\beta} - \\frac{\\alpha}{m}\\v{X}^T(\\v{X}\\v{\\beta} - \\v{y})$$\n",
    "     </li>\n",
    "     <li>(BTW, this is vectorized. Naive loop implementations are wrong: they lose the\n",
    "         <em>simultaneous</em> update of the $\\v{\\beta}_j$.)\n",
    "     </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Gradient descent algorithm</h2>\n",
    "<ul>\n",
    "    <li>Pseudocode (in fact, this is for <b>batch gradient descent</b>, see later):\n",
    "        <ul style=\"background: lightgrey; list-style: none\">\n",
    "            <li>initialize $\\v{\\beta}$ randomly\n",
    "            <li>\n",
    "                repeat until convergence\n",
    "                <ul>\n",
    "                    <li>\n",
    "                        $\\v{\\beta} \\gets \\v{\\beta} - \\frac{\\alpha}{m}\\v{X}^T(\\v{X}\\v{\\beta} - \\v{y})$\n",
    "                    </li>\n",
    "                </ul>\n",
    "             </li>\n",
    "        </ul>\n",
    "    </li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Baby steps</h2>\n",
    "<ul>\n",
    "    <li>We'll use  an example with a single feature/single parameter $\\beta_1$ in order to visualize.</li>\n",
    "    <li>We update $\\beta_1$ gradually, one baby step at a time, unitl the algorithm converges on minimum loss:\n",
    "        <figure>\n",
    "            <img src=\"images/baby_steps1.png\" />\n",
    "        </figure>\n",
    "    </li>\n",
    "    <li>The size of the steps is determined by <!--a <b>hyperparameter</b> called--> the learning rate.\n",
    "    <!--\n",
    "        <ul>\n",
    "            <li>(Hyperparamters are explained in CS4619)</li>\n",
    "        </ul>\n",
    "       -->\n",
    "    </li>\n",
    "    <li>If the learning rate is too small, it will take many updates until convergence:\n",
    "        <figure>\n",
    "            <img src=\"images/baby_steps2.png\" />\n",
    "        </figure>\n",
    "    </li>\n",
    "    <li>If the learning rate is too big, the algorithm might jump across the valley &mdash; it may even end up with\n",
    "        higher loss than before, making the next step bigger.\n",
    "        <ul>\n",
    "            <li>This might make the algorithm <b>diverge</b>.\n",
    "            </li>\n",
    "        </ul>\n",
    "        <figure>\n",
    "            <img src=\"images/baby_steps3.png\" />\n",
    "        </figure>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Why we need to scale for Gradient Descent</h2>\n",
    "<ul>\n",
    "    <li>If we are doing OLS regression using the Normal Equation, we do not need to scale the features.\n",
    "        But if we are doing OLS regression using Gradient Descent, we do need to scale the features.\n",
    "    </li>\n",
    "    <li>If features have different ranges, it affects the shape of the 'bowl'.</li>\n",
    "    <li>E.g. features 1 and 2 have similar ranges of values &mdash; a 'bowl':\n",
    "        <figure>\n",
    "            <img src=\"images/scaled.png\" />\n",
    "        </figure>\n",
    "        <ul>\n",
    "            <li>The algorithm goes straight towards the minimum.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>E.g. feature 1 has smaller values than feature 2 &mdash; an elongated 'bowl':\n",
    "        <figure>\n",
    "            <img src=\"images/unscaled.png\" />\n",
    "        </figure>\n",
    "        <ul>\n",
    "            <li>Since feature 1 has smaller values, it takes a larger change in $\\v{\\beta}_1$ to affect \n",
    "                the loss function, which is why it is elongated.\n",
    "            </li>\n",
    "            <li>It takes more steps to get to the minimum &mdash; steeply down but not really towards the\n",
    "                goal, followed by a long march down a nearly flat valley.\n",
    "            </li>\n",
    "            <li>It makes it more difficult to choose a value for the learning rate that avoids divergence:\n",
    "                a value that suits one feature may not suit another.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Variants of Gradient Descent</h2>\n",
    "<ul>\n",
    "    <li>There are, in fact, three variants:\n",
    "        <ul>\n",
    "            <li>Batch Gradient Descent;</li>\n",
    "            <li>Stochastic Gradient Descent; and</li>\n",
    "            <li>Mini-batch Gradient Descent.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Batch Gradient Descent</h1>\n",
    "<ul>\n",
    "    <li>The pseudocode we saw earlier (repeated here for convenience) is Batch Gradient Descent:\n",
    "        <ul style=\"background: lightgrey; list-style: none\">\n",
    "            <li>initialize $\\v{\\beta}$ randomly\n",
    "            <li>\n",
    "                repeat until convergence\n",
    "                <ul>\n",
    "                    <li>\n",
    "                        $\\v{\\beta} \\gets \\v{\\beta} - \\frac{\\alpha}{m}\\v{X}^T(\\v{X}\\v{\\beta} - \\v{y})$\n",
    "                    </li>\n",
    "                </ul>\n",
    "             </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Why is it called <em>Batch</em> Gradient Descent?\n",
    "        <ul>\n",
    "            <li>The update involves a calculation over the <em>entire</em> training set $\\v{X}$\n",
    "                on every iteration.\n",
    "            </li>\n",
    "            <li>This can be slow for large training sets.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Batch Gradient Descent in numpy</h2>\n",
    "<ul>\n",
    "    <li>For the hell of it, let's implement it ourselves.</li>\n",
    "    <li>Again for the purposes of this explanation, we will use the entire dataset as our training set.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function for OLS regression (assumes X contains all 1s in its first column)\n",
    "def J(X, y, beta):\n",
    "    return np.mean((X.dot(beta) - y) ** 2) / 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gradient_descent_for_ols_linear_regression(X, y, alpha, num_iterations):\n",
    "    \n",
    "    m, n = X.shape\n",
    "    beta = np.random.randn(n) \n",
    "    Jvals = np.zeros(num_iterations)\n",
    "    \n",
    "    for iter in range(num_iterations):\n",
    "        beta -= (1.0 * alpha / m) * X.T.dot(X.dot(beta) - y)\n",
    "        Jvals[iter] = J(X, y, beta)\n",
    " \n",
    "    return beta, Jvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pandas to read the CSV file\n",
    "df = pd.read_csv(\"../datasets/dataset_corkA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature-values and the target values \n",
    "X = df[[\"flarea\", \"bdrms\", \"bthrms\"]].values\n",
    "y = df[\"price\"].values\n",
    "\n",
    "# Scale it\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Add the extra column to X\n",
    "X = add_dummy_feature(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.52297328e+02, 1.75215074e+02, 3.50016725e-01, 1.45857121e+00])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the Batch Gradient Descent\n",
    "beta, Jvals = batch_gradient_descent_for_ols_linear_regression(X, y, alpha = 0.03, num_iterations = 500)\n",
    "\n",
    "# Display beta\n",
    "beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Bear in mind that the coefficients it finds are on the scaled data.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>It's a good idea to plot the values of the loss function against the number of iterations.\n",
    "    </li>\n",
    "    <li>For OLS regression done using Batch Gradient Descent, if the loss ever increases, then:\n",
    "        <ul>\n",
    "            <li>\n",
    "                the code might be incorrect; or\n",
    "            </li>\n",
    "            <li>\n",
    "                the value of $\\alpha$ is too big and is causing divergence.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(8,6))\n",
    "plt.title(\"$J$ during learning\")\n",
    "plt.xlabel(\"Number of iterations\")\n",
    "plt.xlim(1, Jvals.size)\n",
    "plt.ylabel(\"$J$\")\n",
    "plt.ylim(3500, 50000)\n",
    "xvals = np.linspace(1, Jvals.size, Jvals.size)\n",
    "plt.scatter(xvals, Jvals)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>The algorithm gives us the problem of choosing the number of iterations.</li>\n",
    "    <li>An alternative is to use a very large number of iterations but exit when the gradient vector\n",
    "        becomes tiny:\n",
    "        <ul>\n",
    "            <li>when its norm becomes smaller than <b>tolerance</b>, $\\eta$.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Here's an interactive version that allows you to choose the value of $\\alpha$ and to decide\n",
    "        whether to scale the data or not.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c7e15ddbfd04082ab0013131c149c89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Checkbox(value=True, description='scale'), Dropdown(description='alpha', options=(('0.00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def bgd(scale=True, alpha=0.03):\n",
    "    # Get the feature-values and the target values \n",
    "    X = df[[\"flarea\", \"bdrms\", \"bthrms\"]].values\n",
    "    y = df[\"price\"].values\n",
    "    # Scale the data, if requested\n",
    "    if scale:\n",
    "        X = StandardScaler().fit_transform(X)\n",
    "    # Add the extra column to X\n",
    "    X = add_dummy_feature(X)\n",
    "    # Run the Batch Gradient Descent\n",
    "    beta, Jvals = batch_gradient_descent_for_ols_linear_regression(X, y, alpha, num_iterations = 3000)\n",
    "    # Display beta\n",
    "    print(\"beta: \", beta)\n",
    "    # Plot loss\n",
    "    fig = plt.figure(figsize=(8,6))\n",
    "    plt.title(\"$J$ during learning\")\n",
    "    plt.xlabel(\"Number of iterations\")\n",
    "    plt.xlim(1, Jvals.size)\n",
    "    plt.ylabel(\"$J$\")\n",
    "    plt.ylim(3500, 50000)\n",
    "    xvals = np.linspace(1, Jvals.size, Jvals.size)\n",
    "    plt.scatter(xvals, Jvals)\n",
    "    plt.show()\n",
    "    \n",
    "interactive_plot = interactive(bgd, {'manual': True}, \n",
    "    scale=True, alpha=[(\"0.00009\", 0.00009), (\"0.0009\", 0.0009), (\"0.009\", 0.009), (\"0.09\", 0.09), (\"0.9\", 0.9)]) \n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>\n",
    "        Some people suggest a variant of Batch Gradient Descent in which the value of $\\alpha$ is decreased\n",
    "        over time, i.e. its value in later iterations is smaller\n",
    "        <ul>\n",
    "            <li>Why do they suggest this? </li>\n",
    "            <li>And why isn't it necessary?\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>(But, we'll revisit this idea in Stochastic Gradient Descent.)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Stochastic Gradient Descent</h1>\n",
    "<ul>\n",
    "    <li>As we saw, in each iteration, Batch Gradient Descent does a calculation on the entire\n",
    "        training set, which, for large training sets, may be slow.\n",
    "    </li>\n",
    "    <li><b>Stochastic Gradient Descent (SGD)</b>:\n",
    "        <ul>\n",
    "            <li>On each iteration, it picks just <em>one</em> training example $\\v{x}$ at random and computes \n",
    "                the gradients on just that\n",
    "                one example\n",
    "                $$\\v{\\beta} \\gets \\v{\\beta} - \\alpha\\v{x}^T(\\v{x}\\v{\\beta} - y)$$\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>This gives huge speed-up.</li>\n",
    "    <li>It enables us to train on huge training sets since only one example needs to be in memory in each iteration.\n",
    "    </li>\n",
    "    <li>But, because it is stochastic (the randomness), the loss will not necessarily decrease on each iteration:\n",
    "        <ul>\n",
    "            <li><em>On average</em>, the loss decreases, but in any one iteration, loss may go up or down.</li>\n",
    "            <li>Eventually, it will get close to the minimum, but it will continue to go up and down a bit.\n",
    "                <ul>\n",
    "                    <li>So, once you stop it, the $\\v{\\beta}$ will be close to the best, but not \n",
    "                        necessarily optimal.\n",
    "                    </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>SGD in scikit-learn</h2>\n",
    "<ul>\n",
    "    <li>The <code>fit</code> method of scikit-learn's <code>SGDRegressor</code> class is doing\n",
    "        what we have described:\n",
    "        <ul>\n",
    "            <li>You must scale the features but it inserts the extra column of 1s for us.</li>\n",
    "            <li>You can supply a <code>learning_rate</code> and lots of other things\n",
    "                (in the code below, we'll just use the defaults).\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>(Again, we'll train on the whole dataset.)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDRegressor()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the feature-values and the target values \n",
    "X = df[[\"flarea\", \"bdrms\", \"bthrms\"]].values\n",
    "y = df[\"price\"].values\n",
    "\n",
    "# Scale it\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Create the SGDRegressor and fit the model\n",
    "sgd = SGDRegressor()\n",
    "sgd.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>SGD in numpy</h2>\n",
    "<ul>\n",
    "    <li>For the hell of it, let's implement a simple version ourselves</li>\n",
    "    <li>(Again, we'll train on the whole dataset.)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent_for_ols_linear_regression(X, y, alpha, num_epochs):\n",
    "    \n",
    "    m, n = X.shape\n",
    "    beta = np.random.randn(n) \n",
    "    Jvals = np.zeros(num_epochs * m)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for i in range(m):\n",
    "            rand_idx = np.random.randint(m)\n",
    "            xi = X[rand_idx:rand_idx + 1]\n",
    "            yi = y[rand_idx:rand_idx + 1]\n",
    "            beta -= alpha * xi.T.dot(xi.dot(beta) - yi)\n",
    "            Jvals[epoch * m + i] = J(X, y, beta)\n",
    " \n",
    "    return beta, Jvals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>(One common alternative to the code above is to shuffle between epochs and remove the randomness within the\n",
    "        inner loop.)\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature-values and the target values \n",
    "X = df[[\"flarea\", \"bdrms\", \"bthrms\"]].values\n",
    "y = df[\"price\"].values\n",
    "\n",
    "# Scale it\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Add the extra column to X\n",
    "X = add_dummy_feature(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([343.65937377, 186.89426081, -25.65953846,   5.03654973])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the Stochastic Gradient Descent\n",
    "beta, Jvals = stochastic_gradient_descent_for_ols_linear_regression(X, y, alpha = 0.03, num_epochs = 50)\n",
    "\n",
    "# Display beta\n",
    "beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(8,6))\n",
    "plt.title(\"$J$ during learning\")\n",
    "plt.xlabel(\"Number of iterations\")\n",
    "plt.xlim(1, Jvals.size)\n",
    "plt.ylabel(\"$J$\")\n",
    "plt.ylim(3500, 50000)\n",
    "xvals = np.linspace(1, Jvals.size, Jvals.size)\n",
    "plt.scatter(xvals, Jvals)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Quite a bumpy ride!</li>\n",
    "    <li>So, let's try <b>simulated annealing</b>.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Simulated Annealing</h2>\n",
    "<ul>\n",
    "    <li>As we discussed, SGD does not settle at the minimum.</li>\n",
    "    <li>One solution is to gradually reduce the learning rate:\n",
    "        <ul>\n",
    "            <li>Updates start out 'large' so you make progress.</li>\n",
    "            <li>But, over time, updates get smaller, allowing SGD to settle at or near the global minimum.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>The function that determines how to reduce the learning rate is called the <b>learning schedule</b>.\n",
    "        <ul>\n",
    "            <li>Reduce it too quickly and you may not converge on or near to the global minimum.</li>\n",
    "            <li>Reduce it too slowly and you may still bounce around a lot and, if stopped after too few iterations, \n",
    "                may end up\n",
    "                with a suboptimal solution.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_schedule(t):\n",
    "    return 5 / (t + 50)\n",
    "    \n",
    "def stochastic_gradient_descent_for_ols_linear_regression_with_simulated_annealing(X, y, num_epochs):\n",
    "    \n",
    "    m, n = X.shape\n",
    "    beta = np.random.randn(n) \n",
    "    Jvals = np.zeros(num_epochs * m)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for i in range(m):\n",
    "            rand_idx = np.random.randint(m)\n",
    "            xi = X[rand_idx:rand_idx + 1]\n",
    "            yi = y[rand_idx:rand_idx + 1]\n",
    "            alpha = learning_schedule(epoch * m + i)\n",
    "            beta -= alpha * xi.T.dot(xi.dot(beta) - yi)\n",
    "            Jvals[epoch * m + i] = J(X, y, beta)\n",
    " \n",
    "    return beta, Jvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.51564761e+02, 1.77656678e+02, 6.22001665e-02, 3.30499098e+00])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the Stochastic Gradient Descent\n",
    "beta, Jvals = stochastic_gradient_descent_for_ols_linear_regression_with_simulated_annealing(X, y, num_epochs = 50)\n",
    "\n",
    "# Display beta\n",
    "beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(8,6))\n",
    "plt.title(\"$J$ during learning\")\n",
    "plt.xlabel(\"Number of iterations\")\n",
    "plt.xlim(1, Jvals.size)\n",
    "plt.ylabel(\"$J$\")\n",
    "plt.ylim(3500, 50000)\n",
    "xvals = np.linspace(1, Jvals.size, Jvals.size)\n",
    "plt.scatter(xvals, Jvals)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Mini-Batch Gradient Descent</h1>\n",
    "<ul>\n",
    "    <li>Batch Gradient Descent computes gradients from the full training set.</li>\n",
    "    <li>Stochastic Gradient Descent computes gradients from just one example.</li>\n",
    "    <li>Mini-Batch Gradient Descent lies between the two:\n",
    "        <ul>\n",
    "            <li>It computes gradients from a small randomly-selected subset of the training set, called a\n",
    "                <b>mini-batch</b>.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Since it lies between the two:\n",
    "        <ul>\n",
    "            <li>It may bounce less and get closer to the global minimum than SGD&hellip;\n",
    "                <ul>\n",
    "                    <li>&hellip;although both of them can reach the global minimum with a good learning schedule.</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>Its time and memory costs lie between the two.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>The Normal Equation versus Gradient Descent</h1>\n",
    "<ul>\n",
    "    <li>Efficiency/scaling-up to large training sets:\n",
    "        <ul>\n",
    "            <li>Normal Equation: \n",
    "                <ul>\n",
    "                    <li>is linear in $m$, so can handle large training sets efficiently if they fit into\n",
    "                        main memory;\n",
    "                    </li>\n",
    "                    <li>but it has to compute the inverse (or psueudo-inverse) of a $n \\times n$ matrix, which takes\n",
    "                        time between quadratic and cubic in $n$, and so is only feasible for smallish $n$ (up to\n",
    "                        a few thousand).\n",
    "                    </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>Gradient Descent:\n",
    "                <ul>\n",
    "                    <li>SGD scales really well to huge $m$;</li>\n",
    "                    <li>All three Gradient Descent methods can handle huge $n$ (even 100s of 1000s).</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Finding the global minimum for OLS regression:\n",
    "        <ul>\n",
    "            <li>Normal Equation: guaranteed to find the global minimum.</li>\n",
    "            <li>Gradient Descent: all a bit dependent on number of iterations, learning rate, learning schedule.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Feature scaling:\n",
    "        <ul>\n",
    "            <li>Normal Equation: scaling is not needed. \n",
    "            </li>\n",
    "            <li>Gradient Descent: scaling <em>is</em> needed.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Finally, Gradient Descent is a general method, whereas the Normal Equation is only for OLS regression.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Non-Convex Functions</h1>\n",
    "<ul>\n",
    "    <li>The loss function for OLS regression is convex and it has a slope that never changes abruptly.\n",
    "        <ul>\n",
    "            <li>This gives us good 'guarantees' about reaching the minimum\n",
    "                (depending on such things as running for long enough, using a learning rate that isn't too high,\n",
    "                and whether we are using Batch, Mini-Batch or Stochastic Gradient Descent).\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>But Gradient Descent is a generic method: you can use it to find the minima of other loss functions.</li>\n",
    "    <li>But not all loss functions are convex, which can cause problems for Gradient Descent:\n",
    "        <figure>\n",
    "            <img src=\"images/local_minima.png\" />\n",
    "        </figure>\n",
    "        <ul>\n",
    "            <li>The algorithm might converge to a local minimum, instead of the global minimum.</li>\n",
    "            <li>It may take a long time to cross a plateau.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>What do we do about this?\n",
    "        <ul>\n",
    "            <li>One thing is to prefer Stochastic Gradient Descent (or Mini-Batch Gradient Descent):\n",
    "                because of the way they 'bounce around', they might even escape a\n",
    "                local minimum, and might even get to the global minimum.\n",
    "            </li>\n",
    "            <li>In this context, simulated annealing is also useful: updates start out 'large' allowing these\n",
    "                algorithms to make \n",
    "                progress and even escape local minima; but, over time, updates get smaller, allowing \n",
    "                these algorithms to settle at or near the global minimum.\n",
    "            </li>\n",
    "            <li>But, if using simulated annealing, if you reduce the learning rate too quickly, you may \n",
    "                stil get stuck in a local minimum.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
