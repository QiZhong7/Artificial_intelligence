{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>CS4618: Artificial Intelligence I</h1>\n",
    "<h1>Neural Network Examples</h1>\n",
    "<h2>\n",
    "    Derek Bridge<br>\n",
    "    School of Computer Science and Information Technology<br>\n",
    "    University College Cork\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Initialization</h1>\n",
    "$\\newcommand{\\Set}[1]{\\{#1\\}}$ \n",
    "$\\newcommand{\\Tuple}[1]{\\langle#1\\rangle}$ \n",
    "$\\newcommand{\\v}[1]{\\pmb{#1}}$ \n",
    "$\\newcommand{\\cv}[1]{\\begin{bmatrix}#1\\end{bmatrix}}$ \n",
    "$\\newcommand{\\rv}[1]{[#1]}$ \n",
    "$\\DeclareMathOperator{\\argmax}{arg\\,max}$ \n",
    "$\\DeclareMathOperator{\\argmin}{arg\\,min}$ \n",
    "$\\DeclareMathOperator{\\dist}{dist}$\n",
    "$\\DeclareMathOperator{\\abs}{abs}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from tensorflow.keras.datasets.mnist import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cork Property Prices Dataset\n",
    "\n",
    "# Use pandas to read the CSV file into a DataFrame\n",
    "cork_df = pd.read_csv(\"../datasets/dataset_corkA.csv\")\n",
    "\n",
    "# Shuffle the dataset\n",
    "cork_df = cork_df.sample(frac=1, random_state=2)\n",
    "cork_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Split off the test set: 20% of the dataset.\n",
    "dev_cork_df, test_cork_df = train_test_split(cork_df, train_size=0.8, random_state=2)\n",
    "\n",
    "# The features \n",
    "cork_features = [\"flarea\", \"bdrms\", \"bthrms\"]\n",
    "\n",
    "# Create the preprocessor\n",
    "cork_preprocessor = ColumnTransformer([\n",
    "        (\"scaler\", StandardScaler(), cork_features)],\n",
    "        remainder=\"passthrough\")\n",
    "\n",
    "# Extract the features but leave as a DataFrame\n",
    "dev_cork_X = dev_cork_df[cork_features]\n",
    "test_cork_X = test_cork_df[cork_features]\n",
    "\n",
    "# Target values, converted to a 1D numpy array\n",
    "dev_cork_y = dev_cork_df[\"price\"].values\n",
    "test_cork_y = test_cork_df[\"price\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CS1109 Dataset\n",
    "\n",
    "# Use pandas to read the CSV file into a DataFrame\n",
    "cs1109_df = pd.read_csv(\"../datasets/dataset_cs1109.csv\")\n",
    "\n",
    "# Shuffle the dataset\n",
    "cs1109_df = cs1109_df.sample(frac=1, random_state=2)\n",
    "cs1109_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Split off the test set: 20% of the dataset. Note the stratification\n",
    "dev_cs1109_df, test_cs1109_df = train_test_split(cs1109_df, train_size=0.8, \n",
    "                                                     stratify=cs1109_df[\"outcome\"], random_state=2)\n",
    "\n",
    "# The features \n",
    "cs1109_features = [\"lect\", \"lab\", \"cao\"]\n",
    "\n",
    "# Create the preprocessor\n",
    "cs1109_preprocessor = ColumnTransformer([\n",
    "        (\"scaler\", StandardScaler(), cs1109_features)],\n",
    "        remainder=\"passthrough\")\n",
    "\n",
    "# Extract the features but leave as a DataFrame\n",
    "dev_cs1109_X = dev_cs1109_df[cs1109_features]\n",
    "test_cs1109_X = test_cs1109_df[cs1109_features]\n",
    "\n",
    "# Target values, encoded and converted to a 1D numpy array\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(cs1109_df[\"outcome\"])\n",
    "dev_cs1109_y = label_encoder.transform(dev_cs1109_df[\"outcome\"])\n",
    "test_cs1109_y = label_encoder.transform(test_cs1109_df[\"outcome\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iris dataset\n",
    "\n",
    "# Load the dataset (a dictionary) and get the features DataFrame and target values from the dictionary\n",
    "iris = load_iris(as_frame=True)\n",
    "iris_df = iris.data\n",
    "iris_y = iris.target\n",
    "\n",
    "# Shuffle the features and the target values in the same way\n",
    "idx = np.random.permutation(iris_df.index)\n",
    "iris_df.reindex(idx)\n",
    "iris_y.reindex(idx)\n",
    "iris_df.reset_index(drop=True, inplace=True)\n",
    "iris_y.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Split off the test set: 20% of the dataset.\n",
    "dev_iris_df, test_iris_df, dev_iris_y, test_iris_y = train_test_split(iris_df, iris_y, train_size=0.8, \n",
    "                                                                              random_state=4)\n",
    "\n",
    "# Create the preprocessor\n",
    "iris_preprocessor = ColumnTransformer([\n",
    "        (\"scaler\", StandardScaler(), iris_df.columns)],\n",
    "        remainder=\"passthrough\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Introduction</h1>\n",
    "<ul>\n",
    "    <li>We'll use layered, dense, feedforward neural networks for regression, binary classification\n",
    "        and multi-class classification:\n",
    "        <ul>\n",
    "            <li>We'll use our three small datasets that contain 'structured' data (sometimes\n",
    "                called 'tabular' data): not necessarily ideal for deep learning.</li>\n",
    "            <li>We'll see one example that uses images.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>This will illustrate some of the different activation functions we can use:\n",
    "        <ul>\n",
    "            <li>in the output layer: linear, sigmoid or softmax; and</li>\n",
    "            <li>in the hidden layers: sigmoid or ReLU.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>This will also introduce the Keras library.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>The Keras library</h1>\n",
    "<ul>\n",
    "    <li>scikit-learn has very limited support for neural networks.</li>\n",
    "    <li>There are now many libraries that do support tensor computation, neural neworks and deep learning \n",
    "         including in Python:\n",
    "        <ul>\n",
    "            <li>Tensorflow, PyTorch, Caffe, Theano, CNTK.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>We will use Keras, which is a high-level API for Tensorflow, first released in 2015\n",
    "        by Fran&ccedil;ois Chollet of Google (<a href=\"https://keras.io\">https://keras.io</a>), which has done\n",
    "        a lot to make Deep Learning accessible to people:\n",
    "        <ul>\n",
    "            <li>It is very high-level, making it easy to construct networks, fit models and make predictions.</li>\n",
    "            <li>The downside is it gives less fine-graned control than TensorFlow itself.</li>\n",
    "            <li>This seems a suitable trade-off for us: our module is about AI, not the intricacies of\n",
    "                TensorFlow.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>For simple neural networks, Keras is even compatible with scikit-learn:\n",
    "        <ul>\n",
    "            <li>Using wrappers, we can have simple neural networks at the end of our pipelines.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Keras concepts</h1>\n",
    "<ul>\n",
    "    <li><b>Layers</b> are the building blocks.\n",
    "        <ul>\n",
    "            <li>To begin with, we will use <b>dense layers</b>.</li>\n",
    "            <li>The activation functions of <em>hidden layers</em> are open for you to choose, \n",
    "                e.g. sigmoid or ReLU.\n",
    "            </li>\n",
    "            <li>But the activation functions of <em>output layers</em> are determined by the task:\n",
    "                <ul>\n",
    "                    <li>Regression: linear activation function (default);</li>\n",
    "                    <li>Binary classification: sigmoid activation function; and</li>\n",
    "                    <li>Multiclass classification: softmax activation function.</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Layers are combined into <b>networks</b>:\n",
    "        <ul>\n",
    "            <li>Consecutive layers must be compatible: the shape of the input to one layer is the shape of \n",
    "                the output of the preceding layer.\n",
    "            </li>\n",
    "            <li>In early lectures, we only consider a stack of layers but Keras allows directed acyclic graphs \n",
    "                and, later, we will briefly discuss some examples that are not just stacks of layers.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Once the network is built, we <b>compile</b> it, specifying:\n",
    "        <ul>\n",
    "            <li>A <b>loss function</b>:\n",
    "                <ul>\n",
    "                    <li>Regression, e.g. mean-squared-error (<code>mse</code>);</li>\n",
    "                    <li>Binary classification, e.g. (binary) cross-entropy (<code>binary_crossentropy</code>);</li>\n",
    "                    <li>Multiclass classification, e.g. (categorical) cross-entropy \n",
    "                        (<code>sparse_categorical_crossentropy</code> if the labels are encoded as\n",
    "                        integer labels\n",
    "                        or <code>categorical_crossentropy</code> if the integer labels are then also one-hot\n",
    "                        encoded).</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>An <b>optimizer</b>, such as SGD &mdash; but see below.</li>\n",
    "            <li>A list of metrics to monitor during training and testing:\n",
    "                <ul>\n",
    "                    <li>Regression, e.g. mean-absolute-error (<code>mae</code>);</li>\n",
    "                    <li>Classification, e.g. accuracy (<code>acc</code>).</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Keras optimizers</h1>\n",
    "<ul>\n",
    "    <li>We know about Gradient Descent: Batch, Mini-Batch, Stochastic.</li>\n",
    "    <li>Without going into details, many other variants of Gradient Descent have been devised:\n",
    "        <ul>\n",
    "            <li>some may have better convergence behaviour in the case of local minima;</li>\n",
    "            <li>some may converge more quickly.</li>\n",
    "        </ul>\n",
    "        although a disadvantage is that they typically introduce further hyperparameters\n",
    "        (e.g. momentum) in addition to learning rate.\n",
    "    </li>\n",
    "    <li>We'll use SGD below.\n",
    "        <ul>\n",
    "            <li>Be aware, the Keras default SGD learning rate is 0.01 &mdash; quite high, \n",
    "                often resulting in divergence, so we must often change it.\n",
    "            </li>\n",
    "            <li>Be aware too that, although this optimizer is called SGD, behaviour depends on the\n",
    "                <code>batch_size</code>. If <code>batch_size</code> is 1, then this is what we earlier\n",
    "                called Stochastic Gradient Descent; if <code>batch_size</code> is equal to the size \n",
    "                of the training set\n",
    "                (excluding the validation set), then we get the equivalent of what we called Batch Gradient\n",
    "                Descent; and if it is somewhere in between (which, in fact, is the most common way\n",
    "                of training models in Keras), then it is Mini-Batch Gradient Descent.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow give warnings that explain that tensorflow was originally compiled on a different computer \n",
    "# architecture from the one you are using. This means its performance may not be optimal.\n",
    "# It explains that, if you want to optimize tensorflow for your architecture, then you need to rebuild (recompile)\n",
    "# tensorflow from scratch. We won't do this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>A Neural Network for Regression</h1>\n",
    "<ul>\n",
    "    <li>For regression on structured/tabular data, we might use a network with the following architecture:\n",
    "        <ul>\n",
    "            <li>Input layer: one input per feature.</li>\n",
    "            <li>Hidden layers: one or more hidden layers.\n",
    "                <ul>\n",
    "                    <li>Activation function for neurons in hidden layers can be the sigmoid function or ReLU.\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>Output layer: just one output neuron (assuming we're predicting a single number).\n",
    "                <ul>\n",
    "                    <li>Activation function for the output neuron should be the <b>linear function</b>: \n",
    "                        $g(z) = z$\n",
    "                    </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>(There are also biases in each layer except the output layer &mdash; Keras will give us these \n",
    "        'for free'.)\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Example: Cork Property Prices</h2>\n",
    "<ul>\n",
    "    <li>We don't want too many hidden layers, nor too many neurons in each hidden layer. Why?</li>\n",
    "    <li>Let's start with this:\n",
    "        <ul>\n",
    "            <li>An input layer with three inputs ($\\mathit{flarea}$, $\\mathit{bdrms}$, \n",
    "                $\\mathit{bthrms}$);\n",
    "            </li>\n",
    "            <li>Two hidden layers, with 64 neurons in each, and ReLU activation function;</li>\n",
    "            <li>An output layer with a single neuron and linear activation function.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(3,))\n",
    "x = Dense(64, activation=\"relu\")(inputs)\n",
    "x = Dense(64, activation=\"relu\")(x)\n",
    "outputs = Dense(1, activation=\"linear\")(x)\n",
    "cork_model = Model(inputs, outputs)\n",
    "\n",
    "cork_model.compile(optimizer=SGD(learning_rate=0.0001), loss=\"mse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Keras has improved its classes for preprocessing of data recently, and even does them as layers\n",
    "        in the network. E.g. there are layers for scaling and one-hot encoding.\n",
    "    </li>\n",
    "    <li>However, when you have structured/tabular data, it remains easiest to preprocess it with scikit-learn.\n",
    "        There is then a wrapper class that allows us to add a function that returns our neural network to the \n",
    "        end of a scikit-learn\n",
    "        pipeline and run our usual scikit-learn methods.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline\n",
    "cork_pipeline = Pipeline([\n",
    "    (\"preprocessor\", cork_preprocessor),\n",
    "    (\"predictor\", KerasRegressor(build_fn=lambda: cork_model, verbose=0, epochs=60, batch_size=32))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x2a5c9cc10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "84.792482601699"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Error estimation\n",
    "# We'll just train on the dev set and test on the test set\n",
    "# If you want to use ShuffleSplit, KFold, cross_val_score and so on, then you can - in the usual way.\n",
    "# But you may get a warning about an inefficiency in your code. We won't worry aboout this warning. \n",
    "\n",
    "cork_pipeline.fit(dev_cork_X, dev_cork_y)\n",
    "\n",
    "mean_absolute_error(test_cork_y, cork_pipeline.predict(test_cork_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Feel free to edit the code, e.g. add or remove hidden layers, change the number of neurons in the\n",
    "        hidden layers, change ReLU to sigmoid, change from SGD to another optimizer, change the learning rate,\n",
    "        change the number of epochs,\n",
    "        or change the batch size.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>A Neural Network for Binary Classification</h1>\n",
    "<ul>\n",
    "    <li>For binary classification, we might use a network with the following architecture:\n",
    "        <ul>\n",
    "            <li>Input layer: one input per feature.</li>\n",
    "            <li>Hidden layers: one or more hidden layers.\n",
    "                <ul>\n",
    "                    <li>Activation function for neurons in hidden layers can be sigmoid or ReLU.</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>Output layer: just one output neuron (for binary classification).\n",
    "                <ul>\n",
    "                    <li>Activation function for the output neuron should be the sigmoid function also. Why?</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Example: CS1109 Dataset</h2>\n",
    "<ul>\n",
    "    <li>Let's start with this:\n",
    "        <ul>\n",
    "            <li>An input layer with 3 inputs ($\\mathit{lect}$, $\\mathit{lab}$, $\\mathit{cao}$).</li>\n",
    "            <li>Two hidden layers, with 64 neurons in each, and ReLU activation function.</li>\n",
    "            <li>An output layer with a single neuron and sigmoid activation function.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(3,))\n",
    "x = Dense(64, activation=\"relu\")(inputs)\n",
    "x = Dense(64, activation=\"relu\")(x)\n",
    "outputs = Dense(1, activation=\"sigmoid\")(x)\n",
    "cs1109_model = Sequential(Model(inputs, outputs)) # For classification, scikit-learn requires me to add Sequential(...)\n",
    "\n",
    "cs1109_model.compile(optimizer=SGD(learning_rate=0.001), loss=\"binary_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline\n",
    "cs1109_pipeline = Pipeline([\n",
    "    (\"preprocessor\", cs1109_preprocessor),\n",
    "    (\"estimator\", KerasClassifier(build_fn=lambda: cs1109_model, verbose=0, epochs=60, batch_size=32))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x2ac1a0790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dgb/miniforge3/envs/tf25/lib/python3.9/site-packages/tensorflow/python/keras/engine/sequential.py:455: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7101449275362319"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy estimation\n",
    "\n",
    "cs1109_pipeline.fit(dev_cs1109_X, dev_cs1109_y)\n",
    "\n",
    "# You may get a warning because scikit-learn is using tensorflow in a deprecated way\n",
    "accuracy_score(test_cs1109_y, cs1109_pipeline.predict(test_cs1109_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>A Neural Network for Multi-Class Classification</h1>\n",
    "<ul>\n",
    "    <li>For multi-class classification, we might use a network with the following architecture:\n",
    "        <ul>\n",
    "            <li>Input layer: one input per feature.</li>\n",
    "            <li>Hidden layers: one or more hidden layers.\n",
    "                <ul>\n",
    "                    <li>Activation function for neurons in hidden layers can be sigmoid or ReLU.</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>Output layer: one output neuron per class.\n",
    "                <ul>\n",
    "                    <li>Activation function for the output neurons should be the softmax function.</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Example: Iris Dataset</h2>\n",
    "<ul>\n",
    "    <li>Let's start with this:\n",
    "        <ul>\n",
    "            <li>An input layer with 4 inputs (petal width and length, and sepal width and length).</li>\n",
    "            <li>Two hidden layers, with 64 neurons in each, and ReLU activation function.</li>\n",
    "            <li>An output layer with three neurons (one for Setosa, Versicolor and Virginica) and \n",
    "                softmax activation function.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(4,))\n",
    "x = Dense(64, activation=\"relu\")(inputs)\n",
    "x = Dense(64, activation=\"relu\")(x)\n",
    "outputs = Dense(3, activation=\"softmax\")(x)\n",
    "iris_model = Sequential(Model(inputs, outputs))\n",
    "\n",
    "iris_model.compile(optimizer=SGD(learning_rate=0.01), loss=\"sparse_categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Note the loss function above:\n",
    "        <ul>\n",
    "            <li><code>sparse_categorical_crossentropy</code> for multi-class classificastion when the classes\n",
    "                are integers, e.g. 0 = one kind of Iris, 1 = another kind, 2 = a third kind (which is what\n",
    "                we have in the Iris dataset).\n",
    "            </li>\n",
    "            <li><code>categorical_cross_entropy</code> for multi-class classification when the classes have\n",
    "                been one-hot encoded.\n",
    "            </li>\n",
    "            <li>(And, as we've seen, <code>binary_crossentropy</code> for binary classification, where the classes\n",
    "                will be 0 or 1.)\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline\n",
    "iris_pipeline = Pipeline([\n",
    "    (\"preprocessor\", iris_preprocessor),\n",
    "    (\"predictor\", KerasClassifier(build_fn=lambda: iris_model, verbose=0, epochs=60, batch_size=32))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dgb/miniforge3/envs/tf25/lib/python3.9/site-packages/tensorflow/python/keras/engine/sequential.py:455: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy estimation\n",
    "\n",
    "iris_pipeline.fit(dev_iris_df, dev_iris_y)\n",
    "\n",
    "# May get same warning\n",
    "accuracy_score(test_iris_y, iris_pipeline.predict(test_iris_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>A Final Example: MNIST</h1>\n",
    "<ul>\n",
    "    <li>MNIST is a classic dataset for multi-class classification.</li>\n",
    "    <li>The task is classification of hand-written digits.\n",
    "        <ul>\n",
    "            <li>Features: 28 pixel by 28 pixel grayscale images of hand-written digits.\n",
    "                <ul>\n",
    "                    <li>The values are integers in $[0, 255]$.</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>Classes: 0 to 9.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Dataset: 70,000 images, so we can safely use holdout, and it is already partitioned:\n",
    "        <ul>\n",
    "            <li>60,000 training images;</li>\n",
    "            <li>10,000 test images.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras has a utility function for downloading it into four Numpy arrays\n",
    "# To get this to work on macOS, I also had to run this in a terminal:\n",
    "# $ /Applications/Python\\ 3.8/Install\\ Certificates.command\n",
    "# You may need something similar\n",
    "(mnist_x_train, mnist_y_train), (mnist_x_test, mnist_y_test) = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('uint8')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_x_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(mnist_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(mnist_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 126 # Change this number to look at other images\n",
    "some_example = mnist_x_train[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,  23,  79, 192, 216, 216,  91,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  13, 209, 252, 253, 252, 252, 227,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  38,\n",
       "        147, 209, 252, 252, 244, 168,  80,  31,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  38, 175,\n",
       "        253, 252, 214, 139,  25,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 198, 253,\n",
       "        255, 209,  25,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 114, 234, 252,\n",
       "        234,  28,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  23, 234, 252, 252,\n",
       "        100,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  29, 252, 252, 151,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0, 154, 253, 253, 128,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0, 253, 252, 233,  22,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  89, 253, 252, 168,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0, 113, 253, 252, 168,   0,\n",
       "          0,  76, 113, 113,  51,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0, 114, 254, 253, 216, 191,\n",
       "        254, 253, 253, 253, 242, 141,  53,   4,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  38, 253, 252, 252, 252,\n",
       "        253, 252, 252, 252, 253, 252, 252, 103,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0, 253, 252, 252, 214,\n",
       "        156,  56,  56, 106, 178, 252, 252, 228,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0, 153, 252, 252, 139,\n",
       "          0,   0,   0,   0,   4,  78, 252, 252, 101,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  76, 231, 253, 253,\n",
       "         76,   0,   0,   0,   4, 128, 253, 253, 114,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  19, 190, 252,\n",
       "        244,  94,  57,  57, 179, 252, 252, 252,  88,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  19, 193,\n",
       "        253, 252, 252, 252, 253, 252, 252, 127,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  13,\n",
       "        153, 252, 252, 252, 253, 177,  52,   3,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the raw data for this image. Warning: large! (28 by 28)\n",
    "some_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Draw it\n",
    "some_example = some_example.reshape(28, 28)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.imshow(some_example, cmap=plt.cm.binary, interpolation=\"nearest\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at its class\n",
    "mnist_y_train[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>We don't really need scikit-learn pipelines this time.</li>\n",
    "    <li>But we do need to reshape:\n",
    "        <ul>\n",
    "            <li>Our training data is in a 3D array of shape (60000, 28, 28).</li>\n",
    "            <li>We change it to a 2D array of shape (60000, 28 * 28).\n",
    "                <ul>\n",
    "                    <li>This 'flattens' the images.</li>\n",
    "                    <li>When working with images, it is often better not to do this. In a future lecture, we'll\n",
    "                        build neural networks that do not require us to flatten.\n",
    "                    </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>Similarlly, the test data.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_x_train = mnist_x_train.reshape((60000, 28 * 28))\n",
    "\n",
    "mnist_x_test = mnist_x_test.reshape((10000, 28 * 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>We'll do a two-layer network:\n",
    "        <ul>\n",
    "            <li>One hidden layer with 512 neurons, using the ReLU activation function.</li>\n",
    "            <li>The output layer will have 10 neurons, one per class, and \n",
    "                will use the softmax activation function.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Prior to those two layers of neurons, we'll have another layer, which will scale:\n",
    "        <ul>\n",
    "            <li>The values in the original datasset are integers in $[0, 255]$.</li>\n",
    "            <li>The Rescaling layer changes them to floats in $[0, 1]$.</li>\n",
    "        </ul>\n",
    "        By doing this in a layer, we don't need a scikit-learn pipeline.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(28 * 28,))\n",
    "x = Rescaling(scale=1./255)(inputs)\n",
    "x = Dense(512, activation=\"relu\")(x)\n",
    "outputs = Dense(10, activation=\"softmax\")(x)\n",
    "mnist_model = Model(inputs, outputs)\n",
    "\n",
    "mnist_model.compile(optimizer=SGD(learning_rate=0.01), loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>We'll use Keras functions for training and testing.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         [(None, 784)]             0         \n",
      "_________________________________________________________________\n",
      "rescaling_1 (Rescaling)      (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mnist_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Make sure you understand all the numbers above!\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.6157 - accuracy: 0.8495\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3234 - accuracy: 0.9104\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2745 - accuracy: 0.9229\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2438 - accuracy: 0.9328\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2207 - accuracy: 0.9388\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2018 - accuracy: 0.9442\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1860 - accuracy: 0.9483\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1725 - accuracy: 0.9521\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1608 - accuracy: 0.9558\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1507 - accuracy: 0.9582\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2c051c1f0>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_model.fit(mnist_x_train, mnist_y_train, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 3ms/step - loss: 0.1487 - accuracy: 0.9569\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9569000601768494"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss, test_acc = mnist_model.evaluate(mnist_x_test, mnist_y_test)\n",
    "test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Compare training accuracy and test accuracy.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Concluding Remarks</h1>\n",
    "<ul>\n",
    "    <li>A few decisions are constrained: number of inputs; number of output neurons; activation\n",
    "        function of output neurons; and (to some extent) loss function.\n",
    "    </li>\n",
    "    <li>But there are numerous hyperparameters (and even more to come!)\n",
    "        <ul>\n",
    "            <li>Even making a good guess at them is more art than science, although this is changing.</li>\n",
    "            <li>On the other hand, grid search or randomized search will make things even slower than they\n",
    "                already are &mdash; and we still have to specify some sensible values for\n",
    "                them to search through.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>There is a considerable risk of overfitting.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
