{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>CS4618: Artificial Intelligence I</h1>\n",
    "<h1>Model Selection</h1>\n",
    "<h2>\n",
    "    Derek Bridge<br>\n",
    "    School of Computer Science and Information Technology<br>\n",
    "    University College Cork\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Initialization</h1>\n",
    "$\\newcommand{\\Set}[1]{\\{#1\\}}$ \n",
    "$\\newcommand{\\Tuple}[1]{\\langle#1\\rangle}$ \n",
    "$\\newcommand{\\v}[1]{\\pmb{#1}}$ \n",
    "$\\newcommand{\\cv}[1]{\\begin{bmatrix}#1\\end{bmatrix}}$ \n",
    "$\\newcommand{\\rv}[1]{[#1]}$ \n",
    "$\\DeclareMathOperator{\\argmax}{arg\\,max}$ \n",
    "$\\DeclareMathOperator{\\argmin}{arg\\,min}$ \n",
    "$\\DeclareMathOperator{\\dist}{dist}$\n",
    "$\\DeclareMathOperator{\\abs}{abs}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from joblib import dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pandas to read the CSV file into a DataFrame\n",
    "df = pd.read_csv(\"../datasets/dataset_corkA.csv\")\n",
    "\n",
    "# Shuffle the dataset\n",
    "df = df.sample(frac=1, random_state=2)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# The features we want to select\n",
    "features = [\"flarea\", \"bdrms\", \"bthrms\"]\n",
    "\n",
    "# Extract the features but leave as a DataFrame\n",
    "X = df[features]\n",
    "\n",
    "# Target values, converted to a 1D numpy array\n",
    "y = df[\"price\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Parameters and Hyperparameters</h1>\n",
    "<ul>\n",
    "    <li>In machine learning, we distinguish between <b>parameters of the model</b> and <b>hyperparameters of\n",
    "        the learning algorithm</b>.\n",
    "    </li>\n",
    "    <li><b>Parameters</b> are the variables whose values the learning algorithm is trying to find:\n",
    "        <ul>\n",
    "            <li>They define the model that the learning algorithm outputs.</li>\n",
    "            <li>E.g. in the case of linear regression, $\\v{\\beta}_0, \\v{\\beta}_1,\\ldots,\\v{\\beta}_n$ are \n",
    "                the parameters.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><b>Hyperparameters</b> are the variables whose values are set by us:\n",
    "        <ul>\n",
    "            <li>These values are inputs to the learning algorithm.</li>\n",
    "            <li>E.g. the number of neighbours, $k$, for the kNN algorithm. (In scikit-learn's\n",
    "                <code>KNeighborRegression</code> class, this variable is called <code>n_neighbors</code>.)\n",
    "            </li>\n",
    "            <li>E.g. if you are using Gradient Descent for linear regression, then the learning rate\n",
    "                ($\\alpha$) and the number of iterations are hyperparameters. (sickit-learn's\n",
    "                <code>SGDRegressor</code> class calls these <code>learning_rate</code> and\n",
    "                <code>max_iter</code>.) If you are also using simulated annealing, then there may be\n",
    "                further hyperparameters.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Since the learning algorithm does not set them, \n",
    "        <ul>\n",
    "            <li>how will we choose the values for hyperparameters that we use \n",
    "                for error estimation?\n",
    "            </li>\n",
    "            <li>and what values will we use in our final model?</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Error Estimation and Model Selection</h1>\n",
    "<ul>\n",
    "    <li>The process by which we find good values for the hyperparameters is called <b>model selection</b>.</li>\n",
    "    <li>The way we evaluate a particular model (to estimate how it will perform on unseen examples) is\n",
    "        called <b>error estimation</b>.\n",
    "    </li>\n",
    "    <li>So what we want to do is use model selection to find good hyperparameter values and then use\n",
    "        error estimation to estimate future performance.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Model Selection and Error Estimation Done Wrong!</h1>\n",
    "<ul>\n",
    "    <li>It is tempting, <em>but wrong</em>, to do the following:\n",
    "        <ul>\n",
    "            <li>Randomly partition the dataset into training set and test set (e.g. 80%-20%).</li>\n",
    "            <li>Train the predictor on the training set using one set of hyperparameter values; \n",
    "                test it on the test set. \n",
    "            </li>\n",
    "            <li>If not happy with the MAE, train the predictor on the training set using a different \n",
    "                set of hyperparameter values; test it on the test set.\n",
    "            </li>\n",
    "            <li>Keep doing this until you are satisfied.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the object that shuffles and splits the data\n",
    "ss = ShuffleSplit(n_splits=1, train_size=0.8, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a preprocessor\n",
    "preprocessor = ColumnTransformer([\n",
    "        (\"scaler\", StandardScaler(), features)], \n",
    "        remainder=\"passthrough\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-91.6344086])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a pipeline that combines the preprocessor with 1NN\n",
    "knn_model = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"predictor\", KNeighborsRegressor(n_neighbors=1))])\n",
    "\n",
    "# Error estimation for k=1\n",
    "cross_val_score(knn_model, X, y, scoring=\"neg_mean_absolute_error\", cv=ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-74.81182796])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a pipeline that combines the preprocessor with 2NN\n",
    "knn_model = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"predictor\", KNeighborsRegressor(n_neighbors=2))])\n",
    "\n",
    "# Error estimation for k=2\n",
    "cross_val_score(knn_model, X, y, scoring=\"neg_mean_absolute_error\", cv=ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-69.84587814])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a pipeline that combines the preprocessor with 3NN\n",
    "knn_model = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"predictor\", KNeighborsRegressor(n_neighbors=3))])\n",
    "\n",
    "# Error estimation for k=3\n",
    "cross_val_score(knn_model, X, y, scoring=\"neg_mean_absolute_error\", cv=ss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>And so on until we're confident we've found a good value for $k$.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>It is just as wrong to use $k$-fold cross-validation in the same way.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-83.64074930619797"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a pipeline that combines the preprocessor with 1NN\n",
    "knn_model = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"predictor\", KNeighborsRegressor(n_neighbors=1))])\n",
    "\n",
    "# Error estimation for k=1\n",
    "np.mean(cross_val_score(knn_model, X, y, scoring=\"neg_mean_absolute_error\", cv=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-73.79824236817763"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a pipeline that combines the preprocessor with 2NN\n",
    "knn_model = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"predictor\", KNeighborsRegressor(n_neighbors=2))])\n",
    "\n",
    "# Error estimation for k=2\n",
    "np.mean(cross_val_score(knn_model, X, y, scoring=\"neg_mean_absolute_error\", cv=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-72.60975948196116"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a pipeline that combines the preprocessor with 3NN\n",
    "knn_model = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"predictor\", KNeighborsRegressor(n_neighbors=3))])\n",
    "\n",
    "# Error estimation for k=3\n",
    "np.mean(cross_val_score(knn_model, X, y, scoring=\"neg_mean_absolute_error\", cv=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Why is this wrong?</h2>\n",
    "<ul>\n",
    "    <li>It's an example of <b>leakage</b>: \n",
    "        <ul>\n",
    "            <li>Information about the test set is being used to develop the model.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>This means that error estimation will often be over optimistic.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Model Selection and Error Estimation Done Right!</h1>\n",
    "<ul>\n",
    "    <li>The simplest approach is to randomly partition the dataset into three, e.g. 60%-20%-20%:\n",
    "        <ul>\n",
    "            <li>training set;</li>\n",
    "            <li><b>validation set</b>;</li>\n",
    "            <li>test set.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Compute the <b>validation errors</b>:\n",
    "        <ul>\n",
    "            <li>Train the predictor on the training set using one set of hyperparameter values; \n",
    "                test it on the <em>validation set</em>.\n",
    "            </li>\n",
    "            <li>If not happy with the MAE, train the predictor on the training set using a different \n",
    "                set of hyperparameter values; \n",
    "                test it on the <em>validation set</em>.\n",
    "            </li>\n",
    "            <li>Keep doing this until you are satisfied.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Model selection: choose the hyperparameter values that gave you the lowest validation error.\n",
    "    </li>\n",
    "    <li>Error estimation: using these hyperparameter values, train the predictor on the union of the training set \n",
    "        and validation set; test this model \n",
    "        on the <em>test set</em>.\n",
    "    </li>\n",
    "    <li>You can tweak and tune your model as much as you like based on validation error. But, during this\n",
    "        process, you must never use the test set. Only when you've done tweaking and tuning should you \n",
    "        test your chosen model on the test set.\n",
    "    </li>\n",
    "    <li>Of course, this method requires an even bigger dataset: one we can split into three large enough \n",
    "        partitions!\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split off the test set: 20% of the dataset.\n",
    "dev_df, test_df = train_test_split(df, train_size=0.8, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the features but leave as a DataFrame\n",
    "dev_X = dev_df[features]\n",
    "test_X = test_df[features]\n",
    "\n",
    "# Target values, converted to a 1D numpy array\n",
    "dev_y = dev_df[\"price\"].values\n",
    "test_y = test_df[\"price\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the object that shuffles and splits the dev data\n",
    "# Why 0.75? Because 0.75 of 80% of the data is 60% of the original dataset.\n",
    "ss = ShuffleSplit(n_splits=1, train_size=0.75, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-88.97849462])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a pipeline that combines the preprocessor with 1NN\n",
    "knn_model = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"predictor\", KNeighborsRegressor(n_neighbors=1))])\n",
    "\n",
    "# Error estimation for k=1\n",
    "cross_val_score(knn_model, dev_X, dev_y, scoring=\"neg_mean_absolute_error\", cv=ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat the previous cell but with different values for k\n",
    "# Let's suppose k=3 is the one with the lowest MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69.84587813620071"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So now with k=3, re-train on the train+validation sets and test on the test set\n",
    "\n",
    "knn_model = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"predictor\", KNeighborsRegressor(n_neighbors=3))])\n",
    "\n",
    "knn_model.fit(dev_X, dev_y)\n",
    "\n",
    "# Error estimation on the test set.\n",
    "mean_absolute_error(test_y, knn_model.predict(test_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Suppose your dataset is not large enough to split into three in this way.</li>\n",
    "    <li>In this case, it is common to use $k$-fold cross-validation for computing the validation errors.</li>\n",
    "    <li>If so, replace <code>cv=ss</code> with <code>cv=10</code>.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-78.99495021337125"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a pipeline that combines the preprocessor with 1NN\n",
    "knn_model = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"predictor\", KNeighborsRegressor(n_neighbors=1))])\n",
    "\n",
    "# Error estimation for k=1\n",
    "np.mean(cross_val_score(knn_model, dev_X, dev_y, scoring=\"neg_mean_absolute_error\", cv=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat the previous cell but with different values for k\n",
    "# Let's suppose k=3 is the one with the lowest MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69.84587813620071"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So now with k=3, re-train on the train+validation sets and test on the test set\n",
    "\n",
    "knn_model = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"predictor\", KNeighborsRegressor(n_neighbors=3))])\n",
    "\n",
    "knn_model.fit(dev_X, dev_y)\n",
    "\n",
    "# Error estimation on the test set.\n",
    "mean_absolute_error(test_y, knn_model.predict(test_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Grid Search</h1>\n",
    "<ul>\n",
    "    <li>When we want to try lots of hyperparameter values, the code becomes quite repetitive.</li>\n",
    "    <li>Instead, we can specify the values we wish to try for each hyperparameter, and <b>grid search</b>\n",
    "        will try all <em>combinations</em> of these values.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=ShuffleSplit(n_splits=1, random_state=2, test_size=None, train_size=0.75),\n",
       "             estimator=Pipeline(steps=[('preprocessor',\n",
       "                                        ColumnTransformer(remainder='passthrough',\n",
       "                                                          transformers=[('scaler',\n",
       "                                                                         StandardScaler(),\n",
       "                                                                         ['flarea',\n",
       "                                                                          'bdrms',\n",
       "                                                                          'bthrms'])])),\n",
       "                                       ('predictor', KNeighborsRegressor())]),\n",
       "             param_grid={'predictor__n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9,\n",
       "                                                    10]},\n",
       "             scoring='neg_mean_absolute_error')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grid Search (using holdout for the validation errors)\n",
    "\n",
    "# Create a pipeline that combines the preprocessor with kNN\n",
    "knn_model = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"predictor\", KNeighborsRegressor())])\n",
    "\n",
    "# Create a dictionary of hyperparameters and values to try\n",
    "param_grid = {\"predictor__n_neighbors\" : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}\n",
    "\n",
    "# Create the grid search object which will find the best hyperparameter values based on validation error\n",
    "gs = GridSearchCV(knn_model, param_grid, scoring=\"neg_mean_absolute_error\", cv=ss)\n",
    "\n",
    "# Run grid search by calling fit\n",
    "gs.fit(dev_X, dev_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'predictor__n_neighbors': 10}, -69.64193548387097)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can then find out all results (gs.cv_results_) or best result (gs.best_score_) or \n",
    "# best hyperparameter values (gs.best_params_)\n",
    "\n",
    "gs.best_params_, gs.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>So now we have done model selection, we can do error estimation.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63.77526881720429"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we re-train on train+validation and test on the test set\n",
    "\n",
    "knn_model.set_params(**gs.best_params_) \n",
    "knn_model.fit(dev_X, dev_y)\n",
    "mean_absolute_error(test_y, knn_model.predict(test_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Again, if our dataset is too small to split into three, we might prefer to use $k$-fold cross-validation \n",
    "        for computing the validation errors in model selection.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10,\n",
       "             estimator=Pipeline(steps=[('preprocessor',\n",
       "                                        ColumnTransformer(remainder='passthrough',\n",
       "                                                          transformers=[('scaler',\n",
       "                                                                         StandardScaler(),\n",
       "                                                                         ['flarea',\n",
       "                                                                          'bdrms',\n",
       "                                                                          'bthrms'])])),\n",
       "                                       ('predictor',\n",
       "                                        KNeighborsRegressor(n_neighbors=10))]),\n",
       "             param_grid={'predictor__n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9,\n",
       "                                                    10]},\n",
       "             scoring='neg_mean_absolute_error')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grid Search (using k-fold cross-validation for the validation errors)\n",
    "\n",
    "# Create the grid search object which will find the best hyperparameter values based on validation error\n",
    "gs = GridSearchCV(knn_model, param_grid, scoring=\"neg_mean_absolute_error\", cv=10)\n",
    "\n",
    "# Run grid search by calling fit\n",
    "gs.fit(dev_X, dev_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67.76344086021506"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we re-train on train+validation and test on the test set\n",
    "\n",
    "knn_model.set_params(**gs.best_params_) \n",
    "knn_model.fit(dev_X, dev_y)\n",
    "mean_absolute_error(test_y, knn_model.predict(test_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Randomized Search</h1>\n",
    "<ul>\n",
    "    <li>When the number of combinations of hyperparameter values is high, Grid Search's exhaustive\n",
    "        approach may take too long.\n",
    "    </li>\n",
    "    <li>We can instead use Randomized Search:\n",
    "        <ul>\n",
    "            <li>Replace <code>GridSearchCV</code> by <code>RandomizedSearchCV</code>.</li>\n",
    "            <li>Supply an extra argument to <code>RandomizedSearchCV</code>: <code>n_iter</code>\n",
    "                is how many combinations to try.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10,\n",
       "                   estimator=Pipeline(steps=[('preprocessor',\n",
       "                                              ColumnTransformer(remainder='passthrough',\n",
       "                                                                transformers=[('scaler',\n",
       "                                                                               StandardScaler(),\n",
       "                                                                               ['flarea',\n",
       "                                                                                'bdrms',\n",
       "                                                                                'bthrms'])])),\n",
       "                                             ('predictor',\n",
       "                                              KNeighborsRegressor(n_neighbors=7))]),\n",
       "                   n_iter=5,\n",
       "                   param_distributions={'predictor__n_neighbors': [1, 2, 3, 4,\n",
       "                                                                   5, 6, 7, 8,\n",
       "                                                                   9, 10]},\n",
       "                   random_state=2, scoring='neg_mean_absolute_error')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Randomized Search (using k-fold cross-validation for the validation errors)\n",
    "\n",
    "# Create the randomized search object which will find the best hyperparameter values based on validation error\n",
    "rs = RandomizedSearchCV(knn_model, param_grid, scoring=\"neg_mean_absolute_error\", cv=10, n_iter=5, random_state=2)\n",
    "\n",
    "# Run grid search by calling fit\n",
    "rs.fit(dev_X, dev_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65.79704301075269"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we re-train on train+validation and test on the test set\n",
    "\n",
    "knn_model.set_params(**rs.best_params_) \n",
    "knn_model.fit(dev_X, dev_y)\n",
    "mean_absolute_error(test_y, knn_model.predict(test_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1>Final Steps</h1>\n",
    "<ul>\n",
    "    <li>So now we could do error estimation for linear regression. (There's no model selection\n",
    "        because there are no hyperparameters.) This will enable us to see which predictor is better:\n",
    "        kNN or linear regression.\n",
    "    </li>\n",
    "    <li>Make sure you use the same splits for this, otherwise the two will not be comparable.\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66.01046377907335"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Error estimation for linear regression\n",
    "\n",
    "linear_model = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"predictor\", LinearRegression())])\n",
    "linear_model.fit(dev_X, dev_y)\n",
    "mean_absolute_error(test_y, linear_model.predict(test_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>It looks like linear regression is slightly better. So this is the model we might deploy.</li>\n",
    "    <li>If we want to deploy it, train it on the entire dataset and save the model.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/my_model.pkl']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_model.fit(X, y)\n",
    "dump(linear_model, 'models/my_model.pkl') # For this to work, create a folder called models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Nested $k$-Fold Cross Validation (Advanced!)</h1>\n",
    "<ul>\n",
    "    <li><strong>You can ignore the rest of this notebook</strong>.</li>\n",
    "    <li>Above we took two approaches depending on the size of our dataset:\n",
    "        <ul>\n",
    "            <li>Big dataset: split off a test set (using <code>train_test_split</code>) and\n",
    "                split the rest into a training and validation set (using\n",
    "                <code>ShuffleSplit</code>).\n",
    "            </li>\n",
    "            <li>Dataset not quite so big: split off a test set (using <code>train_test_split</code>) and\n",
    "                then obtain multiple training and validation sets using k-fold cross-validation.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>It is natural to wonder whether, for even smaller datasets, we could use k-fold-cross-validation\n",
    "        to obtain our test sets as well as our validation sets.\n",
    "    </li>\n",
    "    <li>The answer is: kind-of.</li>\n",
    "    <li>This is called nested $k$-fold cross-validation: it's like a nested for-loop.\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-64.89143745503135"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the grid search object which will find the best hyperparameter values based on validation error\n",
    "gs = GridSearchCV(knn_model, param_grid, scoring=\"neg_mean_absolute_error\", cv=10)\n",
    "\n",
    "# Run grid search repeatedly by using cross-val_score\n",
    "np.mean(cross_val_score(gs, X, y, scoring=\"neg_mean_absolute_error\", cv=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>There's a very subtle catch: this can only be used for error estimation and not for\n",
    "        model selection.\n",
    "        <ul>\n",
    "            <li>The reason is: a possibly different model (set of hyperparamter values) is selected\n",
    "                for each of the outer folds.\n",
    "            </li>\n",
    "            <li>You do not end up with a single winner, so you cannot use this to tell you\n",
    "                what hyperparameter values to use going forward.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>So what use is it?\n",
    "        <ul>\n",
    "            <li>Suppose you are an academic, comparing your new whizzo learning algorithm (you hope) \n",
    "                with existing algorithms.\n",
    "            </li>\n",
    "            <li>You are interested in error estimation, but you don't need to know the winning hyperparameter\n",
    "                values, because you have no intention of 'going live' (deploying a winning model).\n",
    "            </li>\n",
    "            <li>It's important you use the best hyperparameter values for your competitors, otherwise you \n",
    "                can be accused of dishonestly giving your whizzo algorithm an unfair advantage.\n",
    "            </li>\n",
    "            <li>As an academic, you may also have the problem of a small dataset.</li>\n",
    "        </ul>\n",
    "        In this case, nested $k$-fold cross-validation may be exactly what you need for making your comparisons.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
